{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 Peer Review Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a classification model based on decision tree, and tuning parameters to improve model performance in Rattle \n",
    "\n",
    "Use Classification Tree to make a Spam Filter. The data is available in a csv file for you. Its source is Source- DAAG package library R "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Variable        | Description                                                                           |\n",
    "|---- ------------|---------------------------------------------------------------------------------------|\n",
    "| crl.tot|\t|\n",
    "| dollar |\t|\n",
    "| bang |\t|\n",
    "| money |\t|\n",
    "| n000 |\t|\n",
    "| make |\t|\n",
    "| yesno |\t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import count_nonzero, median, mean\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "# Plotly\n",
    "# import plotly.express as px\n",
    "# import plotly.offline as py\n",
    "# import plotly.graph_objs as go\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# import researchpy as rp\n",
    "\n",
    "# import shap\n",
    "# import eli5\n",
    "# from IPython.display import display\n",
    "\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime, timedelta, date\n",
    "\n",
    "#import os\n",
    "#import zipfile\n",
    "\n",
    "import scipy.stats\n",
    "from scipy.stats import zscore\n",
    "from collections import Counter\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder, PolynomialFeatures\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_score\n",
    "\n",
    "# from sklearn.feature_selection import f_regression, f_classif, chi2, RFE, RFECV\n",
    "# from sklearn.feature_selection import mutual_info_regression, mutual_info_classif\n",
    "# from sklearn.feature_selection import VarianceThreshold, GenericUnivariateSelect\n",
    "# from sklearn.feature_selection import SelectFromModel, SelectKBest, SelectPercentile\n",
    "\n",
    "from sklearn.inspection import permutation_importance, plot_partial_dependence\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, mean_absolute_error, mean_squared_error,r2_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, PrecisionRecallDisplay, RocCurveDisplay\n",
    "from sklearn.metrics import auc, f1_score, precision_score, recall_score, roc_auc_score, accuracy_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier, plot_tree\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, HistGradientBoostingClassifier\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "\n",
    "import catboost\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "#sets the default autosave frequency in seconds\n",
    "%autosave 60 \n",
    "sns.set_style('dark')\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "plt.rc('axes', labelsize=14)\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('ytick', labelsize=12)\n",
    "\n",
    "#import imblearn\n",
    "#from imblearn.under_sampling import RandomUnderSampler\n",
    "#from imblearn.over_sampling import RandomOverSampler\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import pickle\n",
    "# from pickle import dump, load\n",
    "\n",
    "\n",
    "# PyCaret\n",
    "from pycaret.classification import *\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns',None)\n",
    "#pd.set_option('display.max_rows',100)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format','{:.2f}'.format)\n",
    "\n",
    "# Ensure results are reproducible\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quick Glance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"spam7.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive Statistical Analysis\n",
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class balance\n",
    "df['yesno'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.hist(bins=50, figsize=(20,25), grid=False, layout=(len(df.columns),2), edgecolor = 'black')\n",
    "plt.suptitle('Histogram Feature Distribution', x=0.5, y=1.02, ha='center', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(figsize=(20,10), color=\"blue\", fontsize = 15)\n",
    "plt.title('BoxPlots Feature Distribution', x=0.5, y=1.02, ha='center', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "sns.countplot(x=df.yesno, data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "sns.heatmap(df.corr(),cmap=\"coolwarm\",annot=True,fmt='.2f',linewidths=2)\n",
    "plt.title(\"Correlation Heatmap\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**==================================================================================================================**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.yesno = le.fit_transform(df.yesno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yesno</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   yesno\n",
       "0   2788\n",
       "1   1813"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check class balance\n",
    "df['yesno'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crl.tot</th>\n",
       "      <th>dollar</th>\n",
       "      <th>bang</th>\n",
       "      <th>money</th>\n",
       "      <th>n000</th>\n",
       "      <th>make</th>\n",
       "      <th>yesno</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>278</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1028</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2259</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>191</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>191</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   crl.tot  dollar  bang  money  n000  make  yesno\n",
       "0      278    0.00  0.78   0.00  0.00  0.00      1\n",
       "1     1028    0.18  0.37   0.43  0.43  0.21      1\n",
       "2     2259    0.18  0.28   0.06  1.16  0.06      1\n",
       "3      191    0.00  0.14   0.00  0.00  0.00      1\n",
       "4      191    0.00  0.14   0.00  0.00  0.00      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've prepared our data and we're ready to model. There's one last step before we can begin. We must split the data into features and target variable, and into training data and test data. We do this using the `train_test_split()` function. We'll put 25% of the data into our test set, and use the remaining 75% to train the model.\n",
    "\n",
    "Notice below that we include the argument `stratify=y`. If our master data has a class split of 80/20, stratifying ensures that this proportion is maintained in both the training and test data. `=y` tells the function that it should use the class ratio found in the `y` variable (our target).\n",
    "\n",
    "The less data you have overall, and the greater your class imbalance, the more important it is to stratify when you split the data. If we didn’t stratify, then the function would split the data randomly, and we could get an unlucky split that doesn’t get any of the minority class in the test data, which means we wouldn’t be able to effectively evaluate our model. Worst of all, we might not even realize what went wrong without doing some detective work.\n",
    "\n",
    "Lastly, we set a random seed so we and others can reproduce our work.\n",
    "\n",
    "<img src=\"trin-test.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,0:6]\n",
    "y = df.iloc[:,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 278.   ,    0.   ,    0.778,    0.   ,    0.   ,    0.   ],\n",
       "        [1028.   ,    0.18 ,    0.372,    0.43 ,    0.43 ,    0.21 ],\n",
       "        [2259.   ,    0.184,    0.276,    0.06 ,    1.16 ,    0.06 ],\n",
       "        ...,\n",
       "        [ 118.   ,    0.   ,    0.   ,    0.   ,    0.   ,    0.3  ],\n",
       "        [  78.   ,    0.   ,    0.   ,    0.   ,    0.   ,    0.96 ],\n",
       "        [  40.   ,    0.   ,    0.125,    0.   ,    0.   ,    0.   ]]),\n",
       " array([1, 1, 1, ..., 0, 0, 0]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.values, y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.70\n",
    "test_ratio = 0.15\n",
    "validation_ratio = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train and test subsets (85% for training, 15% for testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, stratify=y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3910, 6), (691, 6), (3910,), (691,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split to create Train, Validation and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the test subset further into test and validation subsets (50% for testing, 50% for validation)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=validation_ratio/(train_ratio+test_ratio), stratify=y_train, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3220, 6), (690, 6), (3220,), (690,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_valid.shape, y_train.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that you can usually try when modeling is scaling your predictor variables. Some models require you to scale the data in order for them to operate as expected, others don't. Naive Bayes does not require data scaling. However, sometimes packages and libraries need to make assumptions and approximations in their calculations. We're already breaking some of these assumptions by using the `GaussianNB` classifier on this dataset, and it may not be helping that some of our predictor variables are on very different scales. In general, scaling might not improve the model, but it probably won't make it worse. Let's try scaling our data.\n",
    "\n",
    "We'll use a function called `MinMaxScaler`, which we'll import from the `sklearn.preprocessing` module. `MinMaxScaler` normalizes each column so every value falls in the range of [0, 1]. The column's maximum value would scale to 1, and its minimum value would scale to 0. Everything else would fall somewhere between. This is the formula:\n",
    "\n",
    "$${x_{scaled}} = \\frac{x - x_{min}}{x_{max} - x_{min}}$$ \n",
    "\n",
    "To use a scaler, you must fit it to the training data, and transform both the training data _and_ the test data using that same scaler. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important transformations we need to apply to our data is feature scaling.  There are two common ways to get all attributes to have the same scale: min-max scaling and standardization.\n",
    "\n",
    "Min-max scaling (or normalization) is the simplest: values are shifted and rescaled so they end up ranging from 0 to 1. This is done by subtracting the min value and dividing by the max minus min.\n",
    "\n",
    "Standardization is different: first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the standard deviation, so that the resulting distribution has unit variance.\n",
    "\n",
    "Scikit-learn library provides `MinMaxScaler` for normalization and `StandardScaler` for standardization needs. For more information on `scikit-learn` [`MinMaxScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork837-2023-01-01) and [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork837-2023-01-01) please visit their respective documentation websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crl.tot</th>\n",
       "      <th>dollar</th>\n",
       "      <th>bang</th>\n",
       "      <th>money</th>\n",
       "      <th>n000</th>\n",
       "      <th>make</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4191</th>\n",
       "      <td>35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1898</th>\n",
       "      <td>56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4293</th>\n",
       "      <td>123</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3016</th>\n",
       "      <td>24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>288</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1328</th>\n",
       "      <td>339</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3939</th>\n",
       "      <td>171</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3839</th>\n",
       "      <td>155</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058</th>\n",
       "      <td>1045</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4132</th>\n",
       "      <td>49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3220 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      crl.tot  dollar  bang  money  n000  make\n",
       "4191       35    0.00  0.16   0.00  0.00  2.32\n",
       "1898       56    0.00  0.00   0.00  0.00  0.00\n",
       "4293      123    0.00  0.00   0.00  0.00  0.00\n",
       "3016       24    0.00  0.00   0.00  0.00  0.00\n",
       "1418      288    0.89  0.80   0.00  0.00  0.00\n",
       "...       ...     ...   ...    ...   ...   ...\n",
       "1328      339    0.09  1.28   0.00  0.00  0.28\n",
       "3939      171    0.00  0.00   0.00  0.00  0.00\n",
       "3839      155    0.04  0.00   0.00  0.00  0.00\n",
       "1058     1045    0.40  0.98   0.30  0.00  0.10\n",
       "4132       49    0.00  0.10   0.00  0.00  0.63\n",
       "\n",
       "[3220 rows x 6 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = minmax.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = minmax.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00214646, 0.        , 0.0206306 , 0.        , 0.        ,\n",
       "        0.51101322],\n",
       "       [0.00347222, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.00770202, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.00972222, 0.00749625, 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.06590909, 0.0661336 , 0.12676787, 0.024     , 0.        ,\n",
       "        0.02202643],\n",
       "       [0.0030303 , 0.        , 0.01323472, 0.        , 0.        ,\n",
       "        0.13876652]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00094697, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.00486111, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.20940657, 0.0049975 , 0.0059686 , 0.        , 0.03571429,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.00063131, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.00094697, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.02973485, 0.        , 0.00233554, 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A reminder about modeling trade-offs\n",
    "\n",
    "Remember, the decision to use only the champion model to predict on the test data comes with a trade-off. The benefit is that we get a true idea of how we'd expect the model to perform on new, unseen data. The cost of this decision is that, by using the validation scores to both tune hyperparamters _and_ select the champion model, we run the risk of selecting the model that most overfit the validation data. \n",
    "\n",
    "Alternatively, we could have selected our champion model by using all of our tuned models to predict on the test data and choosing the one that performed best. That also would have come with a trade-off. There wouldn't be as much risk of overfitting to the validation data, but by using the test data to select our champion model, we wouldn't get a truly objective idea of how the model would perform on new, unseen data. We would need a new dataset for that, which means we would have had to set more data aside at the beginning, resulting in less data to use to train the model. \n",
    "\n",
    "With sufficient data, a more rigorous approach would be:\n",
    "\n",
    "1. Split the data into training, validation, and test sets\n",
    "2. Tune hyperparameters using cross-validation on the training set\n",
    "3. Use _all_ tuned models to predict on the validation set\n",
    "4. Select a champion model based on performance on the validation set\n",
    "5. Use champion model alone to predict on test data\n",
    "\n",
    "Every modeling decision comes with a trade-off. What's most important is that you're aware of the trade-offs and apply the best reasoning to the task at hand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More rigorous approach:\n",
    "\n",
    "<img src = \"optimal_model_flow.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Model\n",
    "\n",
    "The `DecisionTreeClassifier` has many arguments (model hyperparameters) that can be customized and eventually tune the generated decision tree classifiers. Among these arguments, there are three commonly tuned arguments as follows:\n",
    "- criterion: `gini` or `entropy`, which specifies which criteria to be used when splitting a tree node\n",
    "- max_depth: a numeric value to specify the max depth of the tree. Larger tree depth normally means larger model complexity\n",
    "- min_samples_leaf: The minimal number of samples in leaf nodes. Larger samples in leaf nodes will tend to generate simpler trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt1 = DecisionTreeClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt1pred = dt1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt1pred[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Decision Tree Classifier\\n\")\n",
    "print('Accuracy:', '%.3f' % accuracy_score(y_test, dt1pred))\n",
    "print('Precision:', '%.3f' % precision_score(y_test, dt1pred))\n",
    "print('Recall:', '%.3f' % recall_score(y_test, dt1pred))\n",
    "print('F1 Score:', '%.3f' % f1_score(y_test, dt1pred))\n",
    "print('AUC score:', '%3.f' % roc_auc_score(y_test, dt1pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine which evaluation metric might be best, consider how our model might be wrong. There are two possibilities for bad predictions: \n",
    "  \n",
    "  - **False positives:** When the model predicts a customer **will** churn when in fact they won't\n",
    "  - **False negatives:** When the model predicts a customer will **not** churn when in fact they will     \n",
    "\n",
    "As you know, there are a number of performance metrics aside from accuracy to choose from. Some of these include precision, recall, and F1 score. Let's examine these more closely, beginning with _precision_:\n",
    "\n",
    "$$precision = \\frac{\\text{TP}}{\\text{FP+TP}}$$\n",
    "  </br> \n",
    "\n",
    "And _recall_: \n",
    "\n",
    "$$recall = \\frac{\\text{TP}}{\\text{FN+TP}}$$  \n",
    "  </br>\n",
    "  \n",
    "<img src = \"confusion matrix.png\">\n",
    "\n",
    "Precision represents the percentage of all our model's predicted positives that are true positives. This might not be the best metric for us to use, because it disincentivizes predicting someone will churn unless there is a high degree of certainty that they will. This could translate to a high rate of false negatives.\n",
    "\n",
    "On the other hand, recall represents the percentage of all actual positives that the model identifies as such. This also might not be the best metric to use, because it rewards predicting someone will churn even if the likelihood of their doing so is very small. This could translate to a high rate of false positives.\n",
    "\n",
    "So which is worse, false positives or false negatives? Well, we'd first have to define what _worse_ means. This is dependent on the details of the project that you're working on. For the sake of this exercise, let us suppose that we're defining it as the error that would cost the bank more money.\n",
    "\n",
    "Since we don't know the exact cost of predicting a false negative, we'll make an assumption for this exercise. We'll assume that a metric that balances precision and recall is best. The metric that helps us achieve this balance is _F1 score_, which is defined as the harmonic mean of precision and recall. \n",
    "\n",
    "$${F_{1}} = 2 \\cdot \\frac{precision \\cdot  recall}{precision + recall}$$  \n",
    "</br>\n",
    "Again, there are many metrics to choose from. The important thing is that you make an informed decision that is based on your use case.\n",
    "\n",
    "**Question:** What are the four basic parameters for evaluating the performance of a classification model?\n",
    "\n",
    "1. True positives (TP): These are correctly predicted positive values, which means the value of actual and predicted classes are positive. \n",
    "\n",
    "2. True negatives (TN): These are correctly predicted negative values, which means the value of the actual and predicted classes are negative.\n",
    "\n",
    "3. False positives (FP): This occurs when the value of the actual class is negative and the value of the predicted class is positive.\n",
    "\n",
    "4. False negatives (FN): This occurs when the value of the actual class is positive and the value of the predicted class in negative. \n",
    "\n",
    "**Reminder:** When fitting and tuning classification modeld, data professioals aim to minimize false positives and false negatives.\n",
    "\n",
    "**Question:**  What do the four scores demonstrate about your model, and how do you calculate them?\n",
    "\n",
    "- Accuracy (TP+TN/TP+FP+FN+TN): The ratio of correctly predicted observations to total observations. \n",
    " \n",
    "- Precision (TP/TP+FP): The ratio of correctly predicted positive observations to total predicted positive observations. \n",
    "\n",
    "- Recall (Sensitivity, TP/TP+FN): The ratio of correctly predicted positive observations to all observations in actual class.\n",
    "\n",
    "- F1 score: The harmonic average of precision and recall, which takes into account both false positives and false negatives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt1cm = confusion_matrix(y_test, dt1pred)\n",
    "dt1cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, dt1pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(estimator=dt1, X=X_test, y=y_test, ax=ax, display_labels=[\"No\",\"Yes\"])\n",
    "ax.set_title('Confusion matrix of the classifier', size=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "RocCurveDisplay.from_estimator(estimator=dt1, X=X_test, y=y_test, ax=ax)\n",
    "ax.set_title('ROC Curve of the classifier', size=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the tree\n",
    "\n",
    "Next, let's examine the splits of the tree. We'll do this by using the `plot_tree()` function that we imported. We pass to it our fit model as well as some additional parameters. Note that if we did not set `max_depth=2`, the function would return a plot of the entire tree, all the way down to the leaf nodes. This is intractable and unnecessary. We're most interested in the splits nearest the root, because these tell us the most predictive features.\n",
    "\n",
    "`class_names` displays what the majority class of each node is, and `filled` colors the nodes according to their majority class. \n",
    "\n",
    "Note that this plot represents how the tree grew from the _training data_. To make its predictions on the test data, the tree would simply pass each customer in the test data through its splits, from the root node all the way down to a leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,12))\n",
    "plot_tree(dt1, max_depth=2, feature_names=X.columns, class_names=['0','1'], fontsize=14, filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we read this plot? The first line of information in each node is the feature and split point that the model identified as being most predictive. In other words, this is the question that is being asked at that split. For our root node, the question was: _Is the customer less than or equal to 42.5 years old?_\n",
    "\n",
    "At each node, if the answer to the question it asks is \"yes,\" the sample would move to the child node on the left. If the answer is \"no,\" the sample would go to the child node on the right.\n",
    "\n",
    "`gini` refers to the node's _Gini impurity_. This is a way of measuring how \"pure\" a node is. The value can range from 0 to 0.5. A Gini score of 0 means there is no impurity&mdash;the node is a leaf, and all of its samples are of a single class. A score of 0.5 means the classes are all equally represented in that node.\n",
    "\n",
    "`samples` is simply how many samples are in that node, and `value` indicates how many of each class are in the node. Returning to the root node, we have `value = [5972, 1528]`. Notice that these numbers sum to 7,500, which is the number of samples in the node. This tells us that 5,972 customers in this node stayed (y=0) and 1,528 customers churned (y=1).\n",
    "\n",
    "Lastly, we have `class`. This tells us the majority class of the samples in each node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a feature importance graph\n",
    "\n",
    "Uncover which features might be most important to your decision tree model by building a feature importance graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = dt1.feature_importances_\n",
    "\n",
    "feature_importances = pd.Series(importances, index=X.columns)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "feature_importances.plot.bar(ax=ax, figsize=(10,5))\n",
    "ax.set_title(\"Decision Tree Feature Importances\")\n",
    "ax.tick_params('x', rotation=30)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_df = pd.DataFrame(feature_importances, columns=[\"importances\"])\n",
    "feature_importances_df = feature_importances_df.sort_values(by='importances')\n",
    "feature_importances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "sns.barplot(data=feature_importances_df, x=feature_importances_df.importances, y=feature_importances_df.index, orient='h')\n",
    "\n",
    "ax.set_title(\"Decision Tree: Feature Importances for Employee Leaving\", fontsize=15)\n",
    "\n",
    "ax.set_xlabel(\"Importance\")\n",
    "ax.set_ylabel(\"Feature\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyCaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_92172_row8_col1, #T_92172_row12_col1 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_92172\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_92172_level0_col0\" class=\"col_heading level0 col0\" >Description</th>\n",
       "      <th id=\"T_92172_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_92172_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_92172_row0_col0\" class=\"data row0 col0\" >Session id</td>\n",
       "      <td id=\"T_92172_row0_col1\" class=\"data row0 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_92172_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_92172_row1_col0\" class=\"data row1 col0\" >Target</td>\n",
       "      <td id=\"T_92172_row1_col1\" class=\"data row1 col1\" >yesno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_92172_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_92172_row2_col0\" class=\"data row2 col0\" >Target type</td>\n",
       "      <td id=\"T_92172_row2_col1\" class=\"data row2 col1\" >Binary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_92172_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_92172_row3_col0\" class=\"data row3 col0\" >Original data shape</td>\n",
       "      <td id=\"T_92172_row3_col1\" class=\"data row3 col1\" >(4601, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_92172_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_92172_row4_col0\" class=\"data row4 col0\" >Transformed data shape</td>\n",
       "      <td id=\"T_92172_row4_col1\" class=\"data row4 col1\" >(4601, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_92172_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_92172_row5_col0\" class=\"data row5 col0\" >Transformed train set shape</td>\n",
       "      <td id=\"T_92172_row5_col1\" class=\"data row5 col1\" >(3220, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_92172_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_92172_row6_col0\" class=\"data row6 col0\" >Transformed test set shape</td>\n",
       "      <td id=\"T_92172_row6_col1\" class=\"data row6 col1\" >(1381, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_92172_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_92172_row7_col0\" class=\"data row7 col0\" >Numeric features</td>\n",
       "      <td id=\"T_92172_row7_col1\" class=\"data row7 col1\" >6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_92172_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_92172_row8_col0\" class=\"data row8 col0\" >Preprocess</td>\n",
       "      <td id=\"T_92172_row8_col1\" class=\"data row8 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_92172_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_92172_row9_col0\" class=\"data row9 col0\" >Imputation type</td>\n",
       "      <td id=\"T_92172_row9_col1\" class=\"data row9 col1\" >simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_92172_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_92172_row10_col0\" class=\"data row10 col0\" >Numeric imputation</td>\n",
       "      <td id=\"T_92172_row10_col1\" class=\"data row10 col1\" >mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_92172_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_92172_row11_col0\" class=\"data row11 col0\" >Categorical imputation</td>\n",
       "      <td id=\"T_92172_row11_col1\" class=\"data row11 col1\" >mode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_92172_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_92172_row12_col0\" class=\"data row12 col0\" >Normalize</td>\n",
       "      <td id=\"T_92172_row12_col1\" class=\"data row12 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_92172_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_92172_row13_col0\" class=\"data row13 col0\" >Normalize method</td>\n",
       "      <td id=\"T_92172_row13_col1\" class=\"data row13 col1\" >minmax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_92172_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_92172_row14_col0\" class=\"data row14 col0\" >Fold Generator</td>\n",
       "      <td id=\"T_92172_row14_col1\" class=\"data row14 col1\" >StratifiedKFold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_92172_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_92172_row15_col0\" class=\"data row15 col0\" >Fold Number</td>\n",
       "      <td id=\"T_92172_row15_col1\" class=\"data row15 col1\" >5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_92172_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_92172_row16_col0\" class=\"data row16 col0\" >CPU Jobs</td>\n",
       "      <td id=\"T_92172_row16_col1\" class=\"data row16 col1\" >-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_92172_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_92172_row17_col0\" class=\"data row17 col0\" >Use GPU</td>\n",
       "      <td id=\"T_92172_row17_col1\" class=\"data row17 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_92172_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_92172_row18_col0\" class=\"data row18 col0\" >Log Experiment</td>\n",
       "      <td id=\"T_92172_row18_col1\" class=\"data row18 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_92172_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_92172_row19_col0\" class=\"data row19 col0\" >Experiment Name</td>\n",
       "      <td id=\"T_92172_row19_col1\" class=\"data row19 col1\" >clf-default-name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_92172_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_92172_row20_col0\" class=\"data row20 col0\" >USI</td>\n",
       "      <td id=\"T_92172_row20_col1\" class=\"data row20 col1\" >afb5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x235671fc9d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# init setup function\n",
    "s = setup(data=df, target = 'yesno', session_id = 0, normalize=True, normalize_method=\"minmax\",\n",
    "     fold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_218ad th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_218ad_row0_col0, #T_218ad_row0_col3, #T_218ad_row0_col4, #T_218ad_row0_col5, #T_218ad_row1_col0, #T_218ad_row1_col1, #T_218ad_row1_col2, #T_218ad_row1_col3, #T_218ad_row1_col4, #T_218ad_row1_col7, #T_218ad_row2_col0, #T_218ad_row2_col1, #T_218ad_row2_col2, #T_218ad_row2_col3, #T_218ad_row2_col4, #T_218ad_row2_col5, #T_218ad_row2_col6, #T_218ad_row2_col7, #T_218ad_row3_col0, #T_218ad_row3_col1, #T_218ad_row3_col2, #T_218ad_row3_col4, #T_218ad_row3_col5, #T_218ad_row3_col6, #T_218ad_row3_col7, #T_218ad_row4_col0, #T_218ad_row4_col1, #T_218ad_row4_col2, #T_218ad_row4_col3, #T_218ad_row4_col4, #T_218ad_row4_col5, #T_218ad_row4_col6, #T_218ad_row4_col7, #T_218ad_row5_col0, #T_218ad_row5_col1, #T_218ad_row5_col2, #T_218ad_row5_col3, #T_218ad_row5_col4, #T_218ad_row5_col5, #T_218ad_row5_col6, #T_218ad_row5_col7, #T_218ad_row6_col0, #T_218ad_row6_col1, #T_218ad_row6_col2, #T_218ad_row6_col3, #T_218ad_row6_col4, #T_218ad_row6_col5, #T_218ad_row6_col6, #T_218ad_row6_col7, #T_218ad_row7_col0, #T_218ad_row7_col1, #T_218ad_row7_col2, #T_218ad_row7_col3, #T_218ad_row7_col4, #T_218ad_row7_col5, #T_218ad_row7_col6, #T_218ad_row7_col7, #T_218ad_row8_col0, #T_218ad_row8_col1, #T_218ad_row8_col2, #T_218ad_row8_col3, #T_218ad_row8_col4, #T_218ad_row8_col5, #T_218ad_row8_col6, #T_218ad_row8_col7, #T_218ad_row9_col0, #T_218ad_row9_col1, #T_218ad_row9_col2, #T_218ad_row9_col3, #T_218ad_row9_col4, #T_218ad_row9_col5, #T_218ad_row9_col6, #T_218ad_row9_col7, #T_218ad_row10_col0, #T_218ad_row10_col1, #T_218ad_row10_col2, #T_218ad_row10_col3, #T_218ad_row10_col4, #T_218ad_row10_col5, #T_218ad_row10_col6, #T_218ad_row10_col7, #T_218ad_row11_col0, #T_218ad_row11_col1, #T_218ad_row11_col2, #T_218ad_row11_col3, #T_218ad_row11_col4, #T_218ad_row11_col5, #T_218ad_row11_col6, #T_218ad_row11_col7, #T_218ad_row12_col0, #T_218ad_row12_col1, #T_218ad_row12_col2, #T_218ad_row12_col3, #T_218ad_row12_col5, #T_218ad_row12_col6, #T_218ad_row12_col7, #T_218ad_row13_col0, #T_218ad_row13_col1, #T_218ad_row13_col2, #T_218ad_row13_col3, #T_218ad_row13_col4, #T_218ad_row13_col5, #T_218ad_row13_col6, #T_218ad_row13_col7, #T_218ad_row14_col0, #T_218ad_row14_col1, #T_218ad_row14_col2, #T_218ad_row14_col3, #T_218ad_row14_col4, #T_218ad_row14_col5, #T_218ad_row14_col6, #T_218ad_row14_col7, #T_218ad_row15_col0, #T_218ad_row15_col1, #T_218ad_row15_col2, #T_218ad_row15_col3, #T_218ad_row15_col4, #T_218ad_row15_col5, #T_218ad_row15_col6, #T_218ad_row15_col7 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_218ad_row0_col1, #T_218ad_row0_col2, #T_218ad_row0_col6, #T_218ad_row0_col7, #T_218ad_row1_col5, #T_218ad_row1_col6, #T_218ad_row3_col3, #T_218ad_row12_col4 {\n",
       "  text-align: left;\n",
       "  background-color: yellow;\n",
       "}\n",
       "#T_218ad_row0_col8, #T_218ad_row1_col8, #T_218ad_row2_col8, #T_218ad_row3_col8, #T_218ad_row4_col8, #T_218ad_row5_col8, #T_218ad_row6_col8, #T_218ad_row7_col8, #T_218ad_row8_col8, #T_218ad_row10_col8, #T_218ad_row11_col8, #T_218ad_row12_col8, #T_218ad_row13_col8, #T_218ad_row14_col8, #T_218ad_row15_col8 {\n",
       "  text-align: left;\n",
       "  background-color: lightgrey;\n",
       "}\n",
       "#T_218ad_row9_col8 {\n",
       "  text-align: left;\n",
       "  background-color: yellow;\n",
       "  background-color: lightgrey;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_218ad\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_218ad_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_218ad_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_218ad_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
       "      <th id=\"T_218ad_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_218ad_level0_col4\" class=\"col_heading level0 col4\" >Prec.</th>\n",
       "      <th id=\"T_218ad_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
       "      <th id=\"T_218ad_level0_col6\" class=\"col_heading level0 col6\" >Kappa</th>\n",
       "      <th id=\"T_218ad_level0_col7\" class=\"col_heading level0 col7\" >MCC</th>\n",
       "      <th id=\"T_218ad_level0_col8\" class=\"col_heading level0 col8\" >TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_218ad_level0_row0\" class=\"row_heading level0 row0\" >catboost</th>\n",
       "      <td id=\"T_218ad_row0_col0\" class=\"data row0 col0\" >CatBoost Classifier</td>\n",
       "      <td id=\"T_218ad_row0_col1\" class=\"data row0 col1\" >0.8829</td>\n",
       "      <td id=\"T_218ad_row0_col2\" class=\"data row0 col2\" >0.9239</td>\n",
       "      <td id=\"T_218ad_row0_col3\" class=\"data row0 col3\" >0.7864</td>\n",
       "      <td id=\"T_218ad_row0_col4\" class=\"data row0 col4\" >0.9040</td>\n",
       "      <td id=\"T_218ad_row0_col5\" class=\"data row0 col5\" >0.8411</td>\n",
       "      <td id=\"T_218ad_row0_col6\" class=\"data row0 col6\" >0.7491</td>\n",
       "      <td id=\"T_218ad_row0_col7\" class=\"data row0 col7\" >0.7537</td>\n",
       "      <td id=\"T_218ad_row0_col8\" class=\"data row0 col8\" >1.1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_218ad_level0_row1\" class=\"row_heading level0 row1\" >lightgbm</th>\n",
       "      <td id=\"T_218ad_row1_col0\" class=\"data row1 col0\" >Light Gradient Boosting Machine</td>\n",
       "      <td id=\"T_218ad_row1_col1\" class=\"data row1 col1\" >0.8823</td>\n",
       "      <td id=\"T_218ad_row1_col2\" class=\"data row1 col2\" >0.9182</td>\n",
       "      <td id=\"T_218ad_row1_col3\" class=\"data row1 col3\" >0.7998</td>\n",
       "      <td id=\"T_218ad_row1_col4\" class=\"data row1 col4\" >0.8904</td>\n",
       "      <td id=\"T_218ad_row1_col5\" class=\"data row1 col5\" >0.8426</td>\n",
       "      <td id=\"T_218ad_row1_col6\" class=\"data row1 col6\" >0.7491</td>\n",
       "      <td id=\"T_218ad_row1_col7\" class=\"data row1 col7\" >0.7518</td>\n",
       "      <td id=\"T_218ad_row1_col8\" class=\"data row1 col8\" >0.2640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_218ad_level0_row2\" class=\"row_heading level0 row2\" >gbc</th>\n",
       "      <td id=\"T_218ad_row2_col0\" class=\"data row2 col0\" >Gradient Boosting Classifier</td>\n",
       "      <td id=\"T_218ad_row2_col1\" class=\"data row2 col1\" >0.8817</td>\n",
       "      <td id=\"T_218ad_row2_col2\" class=\"data row2 col2\" >0.9230</td>\n",
       "      <td id=\"T_218ad_row2_col3\" class=\"data row2 col3\" >0.7959</td>\n",
       "      <td id=\"T_218ad_row2_col4\" class=\"data row2 col4\" >0.8926</td>\n",
       "      <td id=\"T_218ad_row2_col5\" class=\"data row2 col5\" >0.8412</td>\n",
       "      <td id=\"T_218ad_row2_col6\" class=\"data row2 col6\" >0.7474</td>\n",
       "      <td id=\"T_218ad_row2_col7\" class=\"data row2 col7\" >0.7508</td>\n",
       "      <td id=\"T_218ad_row2_col8\" class=\"data row2 col8\" >0.2740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_218ad_level0_row3\" class=\"row_heading level0 row3\" >et</th>\n",
       "      <td id=\"T_218ad_row3_col0\" class=\"data row3 col0\" >Extra Trees Classifier</td>\n",
       "      <td id=\"T_218ad_row3_col1\" class=\"data row3 col1\" >0.8755</td>\n",
       "      <td id=\"T_218ad_row3_col2\" class=\"data row3 col2\" >0.9112</td>\n",
       "      <td id=\"T_218ad_row3_col3\" class=\"data row3 col3\" >0.8124</td>\n",
       "      <td id=\"T_218ad_row3_col4\" class=\"data row3 col4\" >0.8636</td>\n",
       "      <td id=\"T_218ad_row3_col5\" class=\"data row3 col5\" >0.8372</td>\n",
       "      <td id=\"T_218ad_row3_col6\" class=\"data row3 col6\" >0.7365</td>\n",
       "      <td id=\"T_218ad_row3_col7\" class=\"data row3 col7\" >0.7375</td>\n",
       "      <td id=\"T_218ad_row3_col8\" class=\"data row3 col8\" >0.3980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_218ad_level0_row4\" class=\"row_heading level0 row4\" >xgboost</th>\n",
       "      <td id=\"T_218ad_row4_col0\" class=\"data row4 col0\" >Extreme Gradient Boosting</td>\n",
       "      <td id=\"T_218ad_row4_col1\" class=\"data row4 col1\" >0.8745</td>\n",
       "      <td id=\"T_218ad_row4_col2\" class=\"data row4 col2\" >0.9169</td>\n",
       "      <td id=\"T_218ad_row4_col3\" class=\"data row4 col3\" >0.7920</td>\n",
       "      <td id=\"T_218ad_row4_col4\" class=\"data row4 col4\" >0.8778</td>\n",
       "      <td id=\"T_218ad_row4_col5\" class=\"data row4 col5\" >0.8326</td>\n",
       "      <td id=\"T_218ad_row4_col6\" class=\"data row4 col6\" >0.7327</td>\n",
       "      <td id=\"T_218ad_row4_col7\" class=\"data row4 col7\" >0.7353</td>\n",
       "      <td id=\"T_218ad_row4_col8\" class=\"data row4 col8\" >0.2740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_218ad_level0_row5\" class=\"row_heading level0 row5\" >rf</th>\n",
       "      <td id=\"T_218ad_row5_col0\" class=\"data row5 col0\" >Random Forest Classifier</td>\n",
       "      <td id=\"T_218ad_row5_col1\" class=\"data row5 col1\" >0.8720</td>\n",
       "      <td id=\"T_218ad_row5_col2\" class=\"data row5 col2\" >0.9155</td>\n",
       "      <td id=\"T_218ad_row5_col3\" class=\"data row5 col3\" >0.8109</td>\n",
       "      <td id=\"T_218ad_row5_col4\" class=\"data row5 col4\" >0.8571</td>\n",
       "      <td id=\"T_218ad_row5_col5\" class=\"data row5 col5\" >0.8332</td>\n",
       "      <td id=\"T_218ad_row5_col6\" class=\"data row5 col6\" >0.7295</td>\n",
       "      <td id=\"T_218ad_row5_col7\" class=\"data row5 col7\" >0.7305</td>\n",
       "      <td id=\"T_218ad_row5_col8\" class=\"data row5 col8\" >0.3920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_218ad_level0_row6\" class=\"row_heading level0 row6\" >ada</th>\n",
       "      <td id=\"T_218ad_row6_col0\" class=\"data row6 col0\" >Ada Boost Classifier</td>\n",
       "      <td id=\"T_218ad_row6_col1\" class=\"data row6 col1\" >0.8655</td>\n",
       "      <td id=\"T_218ad_row6_col2\" class=\"data row6 col2\" >0.9167</td>\n",
       "      <td id=\"T_218ad_row6_col3\" class=\"data row6 col3\" >0.7856</td>\n",
       "      <td id=\"T_218ad_row6_col4\" class=\"data row6 col4\" >0.8613</td>\n",
       "      <td id=\"T_218ad_row6_col5\" class=\"data row6 col5\" >0.8216</td>\n",
       "      <td id=\"T_218ad_row6_col6\" class=\"data row6 col6\" >0.7140</td>\n",
       "      <td id=\"T_218ad_row6_col7\" class=\"data row6 col7\" >0.7162</td>\n",
       "      <td id=\"T_218ad_row6_col8\" class=\"data row6 col8\" >0.2080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_218ad_level0_row7\" class=\"row_heading level0 row7\" >knn</th>\n",
       "      <td id=\"T_218ad_row7_col0\" class=\"data row7 col0\" >K Neighbors Classifier</td>\n",
       "      <td id=\"T_218ad_row7_col1\" class=\"data row7 col1\" >0.8559</td>\n",
       "      <td id=\"T_218ad_row7_col2\" class=\"data row7 col2\" >0.8935</td>\n",
       "      <td id=\"T_218ad_row7_col3\" class=\"data row7 col3\" >0.7588</td>\n",
       "      <td id=\"T_218ad_row7_col4\" class=\"data row7 col4\" >0.8596</td>\n",
       "      <td id=\"T_218ad_row7_col5\" class=\"data row7 col5\" >0.8058</td>\n",
       "      <td id=\"T_218ad_row7_col6\" class=\"data row7 col6\" >0.6920</td>\n",
       "      <td id=\"T_218ad_row7_col7\" class=\"data row7 col7\" >0.6956</td>\n",
       "      <td id=\"T_218ad_row7_col8\" class=\"data row7 col8\" >0.0860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_218ad_level0_row8\" class=\"row_heading level0 row8\" >dt</th>\n",
       "      <td id=\"T_218ad_row8_col0\" class=\"data row8 col0\" >Decision Tree Classifier</td>\n",
       "      <td id=\"T_218ad_row8_col1\" class=\"data row8 col1\" >0.8398</td>\n",
       "      <td id=\"T_218ad_row8_col2\" class=\"data row8 col2\" >0.8234</td>\n",
       "      <td id=\"T_218ad_row8_col3\" class=\"data row8 col3\" >0.7801</td>\n",
       "      <td id=\"T_218ad_row8_col4\" class=\"data row8 col4\" >0.8073</td>\n",
       "      <td id=\"T_218ad_row8_col5\" class=\"data row8 col5\" >0.7932</td>\n",
       "      <td id=\"T_218ad_row8_col6\" class=\"data row8 col6\" >0.6625</td>\n",
       "      <td id=\"T_218ad_row8_col7\" class=\"data row8 col7\" >0.6630</td>\n",
       "      <td id=\"T_218ad_row8_col8\" class=\"data row8 col8\" >0.0620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_218ad_level0_row9\" class=\"row_heading level0 row9\" >svm</th>\n",
       "      <td id=\"T_218ad_row9_col0\" class=\"data row9 col0\" >SVM - Linear Kernel</td>\n",
       "      <td id=\"T_218ad_row9_col1\" class=\"data row9 col1\" >0.8040</td>\n",
       "      <td id=\"T_218ad_row9_col2\" class=\"data row9 col2\" >0.0000</td>\n",
       "      <td id=\"T_218ad_row9_col3\" class=\"data row9 col3\" >0.5650</td>\n",
       "      <td id=\"T_218ad_row9_col4\" class=\"data row9 col4\" >0.9024</td>\n",
       "      <td id=\"T_218ad_row9_col5\" class=\"data row9 col5\" >0.6940</td>\n",
       "      <td id=\"T_218ad_row9_col6\" class=\"data row9 col6\" >0.5610</td>\n",
       "      <td id=\"T_218ad_row9_col7\" class=\"data row9 col7\" >0.5948</td>\n",
       "      <td id=\"T_218ad_row9_col8\" class=\"data row9 col8\" >0.0480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_218ad_level0_row10\" class=\"row_heading level0 row10\" >lda</th>\n",
       "      <td id=\"T_218ad_row10_col0\" class=\"data row10 col0\" >Linear Discriminant Analysis</td>\n",
       "      <td id=\"T_218ad_row10_col1\" class=\"data row10 col1\" >0.7686</td>\n",
       "      <td id=\"T_218ad_row10_col2\" class=\"data row10 col2\" >0.8934</td>\n",
       "      <td id=\"T_218ad_row10_col3\" class=\"data row10 col3\" >0.4492</td>\n",
       "      <td id=\"T_218ad_row10_col4\" class=\"data row10 col4\" >0.9249</td>\n",
       "      <td id=\"T_218ad_row10_col5\" class=\"data row10 col5\" >0.6043</td>\n",
       "      <td id=\"T_218ad_row10_col6\" class=\"data row10 col6\" >0.4675</td>\n",
       "      <td id=\"T_218ad_row10_col7\" class=\"data row10 col7\" >0.5285</td>\n",
       "      <td id=\"T_218ad_row10_col8\" class=\"data row10 col8\" >0.0700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_218ad_level0_row11\" class=\"row_heading level0 row11\" >lr</th>\n",
       "      <td id=\"T_218ad_row11_col0\" class=\"data row11 col0\" >Logistic Regression</td>\n",
       "      <td id=\"T_218ad_row11_col1\" class=\"data row11 col1\" >0.7624</td>\n",
       "      <td id=\"T_218ad_row11_col2\" class=\"data row11 col2\" >0.8854</td>\n",
       "      <td id=\"T_218ad_row11_col3\" class=\"data row11 col3\" >0.4334</td>\n",
       "      <td id=\"T_218ad_row11_col4\" class=\"data row11 col4\" >0.9220</td>\n",
       "      <td id=\"T_218ad_row11_col5\" class=\"data row11 col5\" >0.5889</td>\n",
       "      <td id=\"T_218ad_row11_col6\" class=\"data row11 col6\" >0.4514</td>\n",
       "      <td id=\"T_218ad_row11_col7\" class=\"data row11 col7\" >0.5152</td>\n",
       "      <td id=\"T_218ad_row11_col8\" class=\"data row11 col8\" >1.7680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_218ad_level0_row12\" class=\"row_heading level0 row12\" >ridge</th>\n",
       "      <td id=\"T_218ad_row12_col0\" class=\"data row12 col0\" >Ridge Classifier</td>\n",
       "      <td id=\"T_218ad_row12_col1\" class=\"data row12 col1\" >0.7624</td>\n",
       "      <td id=\"T_218ad_row12_col2\" class=\"data row12 col2\" >0.0000</td>\n",
       "      <td id=\"T_218ad_row12_col3\" class=\"data row12 col3\" >0.4303</td>\n",
       "      <td id=\"T_218ad_row12_col4\" class=\"data row12 col4\" >0.9281</td>\n",
       "      <td id=\"T_218ad_row12_col5\" class=\"data row12 col5\" >0.5873</td>\n",
       "      <td id=\"T_218ad_row12_col6\" class=\"data row12 col6\" >0.4508</td>\n",
       "      <td id=\"T_218ad_row12_col7\" class=\"data row12 col7\" >0.5167</td>\n",
       "      <td id=\"T_218ad_row12_col8\" class=\"data row12 col8\" >0.0520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_218ad_level0_row13\" class=\"row_heading level0 row13\" >qda</th>\n",
       "      <td id=\"T_218ad_row13_col0\" class=\"data row13 col0\" >Quadratic Discriminant Analysis</td>\n",
       "      <td id=\"T_218ad_row13_col1\" class=\"data row13 col1\" >0.7593</td>\n",
       "      <td id=\"T_218ad_row13_col2\" class=\"data row13 col2\" >0.8679</td>\n",
       "      <td id=\"T_218ad_row13_col3\" class=\"data row13 col3\" >0.4547</td>\n",
       "      <td id=\"T_218ad_row13_col4\" class=\"data row13 col4\" >0.8741</td>\n",
       "      <td id=\"T_218ad_row13_col5\" class=\"data row13 col5\" >0.5981</td>\n",
       "      <td id=\"T_218ad_row13_col6\" class=\"data row13 col6\" >0.4498</td>\n",
       "      <td id=\"T_218ad_row13_col7\" class=\"data row13 col7\" >0.4988</td>\n",
       "      <td id=\"T_218ad_row13_col8\" class=\"data row13 col8\" >0.0660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_218ad_level0_row14\" class=\"row_heading level0 row14\" >nb</th>\n",
       "      <td id=\"T_218ad_row14_col0\" class=\"data row14 col0\" >Naive Bayes</td>\n",
       "      <td id=\"T_218ad_row14_col1\" class=\"data row14 col1\" >0.7587</td>\n",
       "      <td id=\"T_218ad_row14_col2\" class=\"data row14 col2\" >0.8627</td>\n",
       "      <td id=\"T_218ad_row14_col3\" class=\"data row14 col3\" >0.4547</td>\n",
       "      <td id=\"T_218ad_row14_col4\" class=\"data row14 col4\" >0.8715</td>\n",
       "      <td id=\"T_218ad_row14_col5\" class=\"data row14 col5\" >0.5975</td>\n",
       "      <td id=\"T_218ad_row14_col6\" class=\"data row14 col6\" >0.4486</td>\n",
       "      <td id=\"T_218ad_row14_col7\" class=\"data row14 col7\" >0.4970</td>\n",
       "      <td id=\"T_218ad_row14_col8\" class=\"data row14 col8\" >0.0600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_218ad_level0_row15\" class=\"row_heading level0 row15\" >dummy</th>\n",
       "      <td id=\"T_218ad_row15_col0\" class=\"data row15 col0\" >Dummy Classifier</td>\n",
       "      <td id=\"T_218ad_row15_col1\" class=\"data row15 col1\" >0.6059</td>\n",
       "      <td id=\"T_218ad_row15_col2\" class=\"data row15 col2\" >0.5000</td>\n",
       "      <td id=\"T_218ad_row15_col3\" class=\"data row15 col3\" >0.0000</td>\n",
       "      <td id=\"T_218ad_row15_col4\" class=\"data row15 col4\" >0.0000</td>\n",
       "      <td id=\"T_218ad_row15_col5\" class=\"data row15 col5\" >0.0000</td>\n",
       "      <td id=\"T_218ad_row15_col6\" class=\"data row15 col6\" >0.0000</td>\n",
       "      <td id=\"T_218ad_row15_col7\" class=\"data row15 col7\" >0.0000</td>\n",
       "      <td id=\"T_218ad_row15_col8\" class=\"data row15 col8\" >0.0740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x235636d34c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best = compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_44348 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_44348_row0_col0, #T_44348_row0_col3, #T_44348_row1_col0, #T_44348_row1_col1, #T_44348_row1_col2, #T_44348_row1_col4, #T_44348_row1_col5, #T_44348_row1_col6, #T_44348_row1_col7, #T_44348_row2_col0, #T_44348_row2_col1, #T_44348_row2_col2, #T_44348_row2_col3, #T_44348_row2_col4, #T_44348_row2_col5, #T_44348_row2_col6, #T_44348_row2_col7, #T_44348_row3_col0, #T_44348_row3_col1, #T_44348_row3_col2, #T_44348_row3_col3, #T_44348_row3_col4, #T_44348_row3_col5, #T_44348_row3_col6, #T_44348_row3_col7 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_44348_row0_col1, #T_44348_row0_col2, #T_44348_row0_col4, #T_44348_row0_col5, #T_44348_row0_col6, #T_44348_row0_col7, #T_44348_row1_col3 {\n",
       "  text-align: left;\n",
       "  background-color: yellow;\n",
       "}\n",
       "#T_44348_row0_col8, #T_44348_row1_col8, #T_44348_row2_col8 {\n",
       "  text-align: left;\n",
       "  background-color: lightgrey;\n",
       "}\n",
       "#T_44348_row3_col8 {\n",
       "  text-align: left;\n",
       "  background-color: yellow;\n",
       "  background-color: lightgrey;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_44348\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_44348_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_44348_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_44348_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
       "      <th id=\"T_44348_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_44348_level0_col4\" class=\"col_heading level0 col4\" >Prec.</th>\n",
       "      <th id=\"T_44348_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
       "      <th id=\"T_44348_level0_col6\" class=\"col_heading level0 col6\" >Kappa</th>\n",
       "      <th id=\"T_44348_level0_col7\" class=\"col_heading level0 col7\" >MCC</th>\n",
       "      <th id=\"T_44348_level0_col8\" class=\"col_heading level0 col8\" >TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_44348_level0_row0\" class=\"row_heading level0 row0\" >gbc</th>\n",
       "      <td id=\"T_44348_row0_col0\" class=\"data row0 col0\" >Gradient Boosting Classifier</td>\n",
       "      <td id=\"T_44348_row0_col1\" class=\"data row0 col1\" >0.8817</td>\n",
       "      <td id=\"T_44348_row0_col2\" class=\"data row0 col2\" >0.9230</td>\n",
       "      <td id=\"T_44348_row0_col3\" class=\"data row0 col3\" >0.7959</td>\n",
       "      <td id=\"T_44348_row0_col4\" class=\"data row0 col4\" >0.8926</td>\n",
       "      <td id=\"T_44348_row0_col5\" class=\"data row0 col5\" >0.8412</td>\n",
       "      <td id=\"T_44348_row0_col6\" class=\"data row0 col6\" >0.7474</td>\n",
       "      <td id=\"T_44348_row0_col7\" class=\"data row0 col7\" >0.7508</td>\n",
       "      <td id=\"T_44348_row0_col8\" class=\"data row0 col8\" >0.1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44348_level0_row1\" class=\"row_heading level0 row1\" >et</th>\n",
       "      <td id=\"T_44348_row1_col0\" class=\"data row1 col0\" >Extra Trees Classifier</td>\n",
       "      <td id=\"T_44348_row1_col1\" class=\"data row1 col1\" >0.8755</td>\n",
       "      <td id=\"T_44348_row1_col2\" class=\"data row1 col2\" >0.9112</td>\n",
       "      <td id=\"T_44348_row1_col3\" class=\"data row1 col3\" >0.8124</td>\n",
       "      <td id=\"T_44348_row1_col4\" class=\"data row1 col4\" >0.8636</td>\n",
       "      <td id=\"T_44348_row1_col5\" class=\"data row1 col5\" >0.8372</td>\n",
       "      <td id=\"T_44348_row1_col6\" class=\"data row1 col6\" >0.7365</td>\n",
       "      <td id=\"T_44348_row1_col7\" class=\"data row1 col7\" >0.7375</td>\n",
       "      <td id=\"T_44348_row1_col8\" class=\"data row1 col8\" >0.2080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44348_level0_row2\" class=\"row_heading level0 row2\" >rf</th>\n",
       "      <td id=\"T_44348_row2_col0\" class=\"data row2 col0\" >Random Forest Classifier</td>\n",
       "      <td id=\"T_44348_row2_col1\" class=\"data row2 col1\" >0.8720</td>\n",
       "      <td id=\"T_44348_row2_col2\" class=\"data row2 col2\" >0.9155</td>\n",
       "      <td id=\"T_44348_row2_col3\" class=\"data row2 col3\" >0.8109</td>\n",
       "      <td id=\"T_44348_row2_col4\" class=\"data row2 col4\" >0.8571</td>\n",
       "      <td id=\"T_44348_row2_col5\" class=\"data row2 col5\" >0.8332</td>\n",
       "      <td id=\"T_44348_row2_col6\" class=\"data row2 col6\" >0.7295</td>\n",
       "      <td id=\"T_44348_row2_col7\" class=\"data row2 col7\" >0.7305</td>\n",
       "      <td id=\"T_44348_row2_col8\" class=\"data row2 col8\" >0.2080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44348_level0_row3\" class=\"row_heading level0 row3\" >dt</th>\n",
       "      <td id=\"T_44348_row3_col0\" class=\"data row3 col0\" >Decision Tree Classifier</td>\n",
       "      <td id=\"T_44348_row3_col1\" class=\"data row3 col1\" >0.8398</td>\n",
       "      <td id=\"T_44348_row3_col2\" class=\"data row3 col2\" >0.8234</td>\n",
       "      <td id=\"T_44348_row3_col3\" class=\"data row3 col3\" >0.7801</td>\n",
       "      <td id=\"T_44348_row3_col4\" class=\"data row3 col4\" >0.8073</td>\n",
       "      <td id=\"T_44348_row3_col5\" class=\"data row3 col5\" >0.7932</td>\n",
       "      <td id=\"T_44348_row3_col6\" class=\"data row3 col6\" >0.6625</td>\n",
       "      <td id=\"T_44348_row3_col7\" class=\"data row3 col7\" >0.6630</td>\n",
       "      <td id=\"T_44348_row3_col8\" class=\"data row3 col8\" >0.0840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2355bc8ebb0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compare_tree_models = compare_models(include = ['dt', 'rf', 'et', 'gbc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selected = create_model('dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model = tune_model(catboost, optimize='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(tuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(tuned_model, plot = 'error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(tuned_model, plot='feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_model(tuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(tuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_model(tuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = finalize_model(tuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_predictions = predict_model(final_model, data=data_unseen)\n",
    "unseen_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validated hyperparameter tuning (GridSearchCV)\n",
    "\n",
    "Cross-validating a model using GridSearchCV can be done in a number of different ways. If you find notebooks online that other people have written, you'll likely soon discover this for yourself. But all variations must fulfill the same general requirements. (Refer to the [GridSearchCV documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) for further reading.)\n",
    "\n",
    "The format presented below is step-wise, making it easier to follow.\n",
    "\n",
    "* Create a dictionary of hyperparameters to search over:\n",
    "\n",
    "  - key = name of hyperparameter (string)\n",
    "  - value = values to search over (list)\n",
    "  \n",
    "* Create a dictionary of scoring metrics to capture. These metrics can be selected from scikit-learn's [built-in options](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter) or custom-defined. For this exercise, we'll capture accuracy, precision, recall, and F1 score so we can examine all of them. The metrics are entered as strings.\n",
    "\n",
    "* Instantiate the classifier (and set the `random_state`)\n",
    "\n",
    "* Instantiate the `GridSearchCV` object. Pass as arguments:\n",
    "  - The classifier (`tuned_decision_tree`)\n",
    "  - The dictionary of hyperparameters to search over (`tree_para`)\n",
    "  - The dictionary of scoring metrics (`scoring`)\n",
    "  - The number of cross-validation folds you want (`cv=5`)\n",
    "  - The scoring metric that you want GridSearch to use when it selects the \"best\" model (i.e., the model that performs best on average over all validation folds) (`refit='f1'`*)\n",
    "\n",
    "    \\* The reason it's called `refit` is because once the algorithm finds the combination of hyperparameters that results in the best average score across all validation folds, it will then refit this model to _all_ of the training data. Remember, up until now, with a 5-fold cross-validation, the model has only ever been fit on 80% (4/5) of the training data, because the remaining 20% was held out as a validation fold.\n",
    "\n",
    "* Fit the data (`X_train`, `y_train`) to the `GridSearchCV` object (`clf`)\n",
    "\n",
    "Depending on the number of different hyperparameters you choose, the number of combinations you search over, the size of your data, and your available computing resources, this could take a long time. \n",
    "\n",
    "Now that the model is fit and cross-validated, we can use the `best_estimator_` attribute to inspect the hyperparameter values that yielded the highest F1 score during cross-validation.\n",
    "\n",
    "The `best_score_` attribute returns the best average F1 score across the different folds among all the combinations of hyperparameters. Note that if we had set `refit='recall'` when we instantiated our `GridSearchCV` object earlier, then calling `best_score_` would return the best recall score, and the best parameters might not be the same as what they are in the above cell, because the model would be selected based on a different metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a dictionary of hyperparameters to search over\n",
    "parameters = {'max_depth': np.arange(2,10,2),\n",
    "             'min_samples_leaf': np.arange(1,10,2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a dictionary of scoring metrics to capture\n",
    "scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the classifier\n",
    "tuned_decision_tree = DecisionTreeClassifier(random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the GridSearch\n",
    "dtgs = GridSearchCV(estimator=tuned_decision_tree, \n",
    "                   param_grid=parameters, \n",
    "                   scoring = scoring, \n",
    "                   cv=5, \n",
    "                   refit=\"roc_auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "%%time\n",
    "dtgs.fit(X_random_train, y_random_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is fit and cross-validated, we can use the `best_estimator_` attribute to inspect the hyperparameter values that yielded the highest score during cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the best model from GridSearch\n",
    "dtgs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtgs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `best_score_` attribute returns the best average score across the different folds among all the combinations of hyperparameters. Note that if we had set `refit='recall'` when we instantiated our `GridSearchCV` object earlier, then calling `best_score_` would return the best recall score, and the best parameters might not be the same as what they are in the above cell, because the model would be selected based on a different metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtgs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also find the data for all models evaluated\n",
    "\n",
    "results = pd.DataFrame(dtgs.cv_results_)\n",
    "\n",
    "print(results.shape)\n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can order the different models based on their performance\n",
    "results.sort_values(by='mean_test_roc_auc', ascending=False, inplace=True)\n",
    "\n",
    "results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "results[[\"param_max_depth\",\"param_min_samples_leaf\",\"mean_test_accuracy\", \"mean_test_recall\",\n",
    "         \"mean_test_precision\",\"mean_test_f1\",\"mean_test_roc_auc\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_results(model_name, model_object):\n",
    "    '''\n",
    "    Accepts as arguments a model name (your choice - string) and\n",
    "    a fit GridSearchCV model object.\n",
    "  \n",
    "    Returns a pandas df with the F1, recall, precision, and accuracy scores\n",
    "    for the model with the best mean F1 score across all validation folds.  \n",
    "    '''\n",
    "\n",
    "    # Get all the results from the CV and put them in a df\n",
    "    cv_results = pd.DataFrame(model_object.cv_results_)\n",
    "\n",
    "    # Isolate the row of the df with the max(mean f1 score)\n",
    "    best_estimator_results = cv_results.iloc[cv_results['mean_test_roc_auc'].idxmax(), :]\n",
    "\n",
    "    # Extract accuracy, precision, recall, and f1 score from that row\n",
    "    f1 = best_estimator_results.mean_test_f1\n",
    "    recall = best_estimator_results.mean_test_recall\n",
    "    precision = best_estimator_results.mean_test_precision\n",
    "    accuracy = best_estimator_results.mean_test_accuracy\n",
    "    rocauc = best_estimator_results.mean_test_roc_auc\n",
    "    \n",
    "    # Create table of results\n",
    "    table = pd.DataFrame()\n",
    "    table = table.append({'Model': model_name,\n",
    "                        'F1': f1,\n",
    "                        'Recall': recall,\n",
    "                        'Precision': precision,\n",
    "                        'Accuracy': accuracy,\n",
    "                        'ROC-AUC' : rocauc\n",
    "                        },\n",
    "                        ignore_index=True\n",
    "                       )\n",
    "  \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_results(model_name=\"Decision Tree GSCV\", model_object=dtgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RandomSearchCV\n",
    "\n",
    "Randomised grid search is very useful in finding near-optimal hyper parameters for any machine learning models.\n",
    "\n",
    "Rules of thumb: with 60 iterations, 95% of the time, best 5% sets of parameters can be found, regardless of grid size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = { 'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "               'max_depth': np.arange(2,11,2),\n",
    "               'min_samples_split': np.arange(2,21,3),\n",
    "               'min_samples_leaf':np.arange(1,11,2)\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_randm = RandomizedSearchCV(estimator=dt, param_distributions = parameters, cv = 5, n_iter = 55, \n",
    "                           n_jobs=2, scoring=scoring, refit='roc_auc', return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dt_randm.fit(X_random_train, y_random_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_randm.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_randm.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_randm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also find the data for all models evaluated\n",
    "\n",
    "results = pd.DataFrame(dt_randm.cv_results_)\n",
    "\n",
    "print(results.shape)\n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can order the different models based on their performance\n",
    "results.sort_values(by='mean_test_roc_auc', ascending=False, inplace=True)\n",
    "\n",
    "results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "results[[\"param_min_samples_split\",\"param_max_depth\",\"param_min_samples_leaf\",\"mean_test_accuracy\", \"mean_test_recall\",\n",
    "         \"mean_test_precision\",\"mean_test_f1\",\"mean_test_roc_auc\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_results(model_name, model_object):\n",
    "    '''\n",
    "    Accepts as arguments a model name (your choice - string) and\n",
    "    a fit GridSearchCV model object.\n",
    "  \n",
    "    Returns a pandas df with the F1, recall, precision, and accuracy scores\n",
    "    for the model with the best mean F1 score across all validation folds.  \n",
    "    '''\n",
    "\n",
    "    # Get all the results from the CV and put them in a df\n",
    "    cv_results = pd.DataFrame(model_object.cv_results_)\n",
    "\n",
    "    # Isolate the row of the df with the max(mean f1 score)\n",
    "    best_estimator_results = cv_results.iloc[cv_results['mean_test_roc_auc'].idxmax(), :]\n",
    "\n",
    "    # Extract accuracy, precision, recall, and f1 score from that row\n",
    "    f1 = best_estimator_results.mean_test_f1\n",
    "    recall = best_estimator_results.mean_test_recall\n",
    "    precision = best_estimator_results.mean_test_precision\n",
    "    accuracy = best_estimator_results.mean_test_accuracy\n",
    "    rocauc = best_estimator_results.mean_test_roc_auc\n",
    "  \n",
    "    # Create table of results\n",
    "    table = pd.DataFrame()\n",
    "    table = table.append({'Model': model_name,\n",
    "                        'F1': f1,\n",
    "                        'Recall': recall,\n",
    "                        'Precision': precision,\n",
    "                        'Accuracy': accuracy,\n",
    "                        'ROC-AUC' : rocauc  \n",
    "                        },\n",
    "                        ignore_index=True\n",
    "                       )\n",
    "  \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function on our model\n",
    "dtrs_result_table = make_results(\"Decision Tree RCV\", dt_randm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrs_result_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt2 = DecisionTreeClassifier(criterion='gini', min_samples_split=11, min_samples_leaf=3, max_depth=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt2pred = dt2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt2pred[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy:', '%.3f' % accuracy_score(y_test, dt2pred))\n",
    "print('Precision:', '%.3f' % precision_score(y_test, dt2pred))\n",
    "print('Recall:', '%.3f' % recall_score(y_test, dt2pred))\n",
    "print('F1 Score:', '%.3f' % f1_score(y_test, dt2pred))\n",
    "print('AUC score:', '%3.f' % roc_auc_score(y_test, dt2pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "When performing supervised machine learning analysis, it is common to withhold a portion of the data to test the final model's performance. This model testing is performed on the 'unseen' data, which the model was not trained on. This withholding of a portion of the dataset for testing is called Cross-Validation. Cross-Validation can also be used to select hyper-parameters and test the final model. In this section, we will focus on the test data only.\n",
    "\n",
    "Cross-Validation also helps avoid over-fitting; a complex model could repeat the labels of the samples that it has just seen and, therefore, would have a perfect score but would fail to predict anything useful on the 'unseen' data. Furthermore, a complex model could just be modeling noise.\n",
    "\n",
    "Cross validation method involves dividing the dataset into 3 parts:\n",
    "\n",
    "*   training set - is a portion of the data used for training the model\n",
    "*   validation set - is a portion of the data used to optimize the hyper-parameters of the model. This will     be illustrated in the next lab\n",
    "*   test set - is a portion of the data used to evaluate if the model generalizes enough to work on the     \n",
    "    data it was not trained on   \n",
    "    \n",
    "`Scikit Learn` library contains many methods that can perform the splitting of the data into training, testing and validation sets. The most popular methods that we will cover in this Jupyter Notebook are:\n",
    "\n",
    "*   train_test_split - creates a single split into train and test sets\n",
    "*   K-fold - creates number of k-fold splits, allowing cross validation\n",
    "*   cross_val_score - evaluates model's score through cross validation\n",
    "\n",
    "[`cross_val_predict`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork783-2023-01-01) is a function that does K-fold cross validation for us, appropriately fitting and transforming at every step of the way.\n",
    "\n",
    "Note that `cross_val_predict` doesn't use the same model for all steps; the predictions for each row are made when that row is in the validation set. We really have the collected results of 3 (i.e. `kf.num_splits`) different models. \n",
    "\n",
    "When we are done, `estimator` is still not fitted. If we want to predict on _new_ data, we still have to train our `estimator`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"cross_validation_diagram.png\">\n",
    "<img src = \"k-fold.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = cross_validate(estimator=dt2, X=X_train, y=y_train, scoring=\"roc_auc\", cv=kf, n_jobs=2, return_train_score=True)\n",
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv[\"train_roc_auc\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv[\"test_roc_auc\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation Score (Estimator can be any model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtcv_pred = cross_val_score(estimator=dt2, X=X_train, y=y_train, scoring=\"roc_auc\", cv=kf, n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtcv_pred[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtcv_pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Tree Classifier ( Note: Future Expansion )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et1 = ExtraTreeClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et1pred = et1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et1pred[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extra Tree Classifier\")\n",
    "print('Accuracy:', '%.3f' % accuracy_score(y_test, et1pred))\n",
    "print('Precision:', '%.3f' % precision_score(y_test, et1pred))\n",
    "print('Recall:', '%.3f' % recall_score(y_test, et1pred))\n",
    "print('F1 Score:', '%.3f' % f1_score(y_test, et1pred))\n",
    "print('AUC score:', '%3.f' % roc_auc_score(y_test, et1pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RandomSearchCV\n",
    "\n",
    "Randomised grid search is very useful in finding near-optimal hyper parameters for any machine learning models.\n",
    "\n",
    "Rules of thumb: with 60 iterations, 95% of the time, best 5% sets of parameters can be found, regardless of grid size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et = ExtraTreeClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = { 'criterion': ['gini', 'entropy'],\n",
    "               'splitter' : ['random','best'],\n",
    "               'max_depth': np.arange(2,11,2),\n",
    "               'min_samples_split': np.arange(2,21,3),\n",
    "               'min_samples_leaf':np.arange(1,11,2)\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_randm = RandomizedSearchCV(estimator=et, param_distributions = parameters, cv = 5, n_iter = 55, \n",
    "                           n_jobs=2, scoring=scoring, refit='roc_auc', return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "et_randm.fit(X_random_train, y_random_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_randm.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_randm.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_randm.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging and Random Forest Models\n",
    "\n",
    "Using **ensemble methods** can greatly improve the results achieved with weak machine learning algorithms, also called **weak learners**. Ensemble methods achieve better performance by aggregating the results of many statistically independent models. This process averages out the errors and produces a final better prediction. \n",
    "\n",
    "In this lab you will work with a widely used ensemble method known as **bootstrap aggregating** or simply **bagging**. Bagging follows a simple procedure:\n",
    "1. N learners (machine learning models) are defined. \n",
    "2. N subsamples of the training data are created by **Bernoulli sampling with replacement**.\n",
    "3. The N learners are trained on the subsamples of the training data.\n",
    "4. The ensemble is scored by averaging, or taking a majority vote, of the predictions from the N learners.\n",
    "\n",
    "**Classification and regression tree models** are most typically used with bagging methods. The most common such algorithm is know as the **random forest**. The random forest method is highly scalable and generally produces good results, even for complex problems. \n",
    "\n",
    "Classification and regression trees tend to be robust to noise or outliers in the training data. This is true for the random forest algorithm as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the classifier\n",
    "rf = RandomForestClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_estimators':np.arange(50,200,50),\n",
    "              'criterion': ('gini','entropy'),\n",
    "              'max_depth': np.arange(2,11,2),\n",
    "              'min_samples_split': np.arange(2,11,2),\n",
    "              'min_samples_leaf': np.arange(1,11,2)\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a dictionary of scoring metrics to capture\n",
    "scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_randm = RandomizedSearchCV(estimator=rf, param_distributions = parameters, cv = 5, n_iter = 55, \n",
    "                           n_jobs=2, scoring=scoring, refit='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rf_randm.fit(X_random_train, y_random_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_randm.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_randm.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_randm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also find the data for all models evaluated\n",
    "\n",
    "results = pd.DataFrame(rf_randm.cv_results_)\n",
    "\n",
    "print(results.shape)\n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can order the different models based on their performance\n",
    "results.sort_values(by='mean_test_roc_auc', ascending=False, inplace=True)\n",
    "\n",
    "results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "results[[\"param_n_estimators\", \"param_min_samples_split\",\"param_max_depth\", \"param_min_samples_leaf\", \n",
    "         \"param_criterion\", \"mean_test_accuracy\", \"mean_test_recall\",\n",
    "         \"mean_test_precision\",\"mean_test_f1\",\"mean_test_roc_auc\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_results(model_name, model_object):\n",
    "    '''\n",
    "    Accepts as arguments a model name (your choice - string) and\n",
    "    a fit GridSearchCV model object.\n",
    "  \n",
    "    Returns a pandas df with the F1, recall, precision, and accuracy scores\n",
    "    for the model with the best mean F1 score across all validation folds.  \n",
    "    '''\n",
    "\n",
    "    # Get all the results from the CV and put them in a df\n",
    "    cv_results = pd.DataFrame(model_object.cv_results_)\n",
    "\n",
    "    # Isolate the row of the df with the max(mean f1 score)\n",
    "    best_estimator_results = cv_results.iloc[cv_results['mean_test_roc_auc'].idxmax(), :]\n",
    "\n",
    "    # Extract accuracy, precision, recall, and f1 score from that row\n",
    "    f1 = best_estimator_results.mean_test_f1\n",
    "    recall = best_estimator_results.mean_test_recall\n",
    "    precision = best_estimator_results.mean_test_precision\n",
    "    accuracy = best_estimator_results.mean_test_accuracy\n",
    "    rocauc = best_estimator_results.mean_test_roc_auc\n",
    "  \n",
    "    # Create table of results\n",
    "    table = pd.DataFrame()\n",
    "    table = table.append({'Model': model_name,\n",
    "                        'F1': f1,\n",
    "                        'Recall': recall,\n",
    "                        'Precision': precision,\n",
    "                        'Accuracy': accuracy,\n",
    "                        'ROC-AUC' : rocauc  \n",
    "                        },\n",
    "                        ignore_index=True\n",
    "                       )\n",
    "  \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function on our model\n",
    "rf_result_table = make_results(\"Random Forest RCV\", rf_randm)\n",
    "\n",
    "rf_result_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf2 = RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=10, min_samples_split=8,\n",
    "                            min_samples_leaf=5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf2_pred = rf2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf2_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = cross_validate(estimator=rf2, X=X_train, y=y_train, scoring=\"roc_auc\", cv=kf, n_jobs=2, return_train_score=True)\n",
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv[\"train_roc_auc\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv[\"test_roc_auc\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfcm = confusion_matrix(y_test,rf2_pred)\n",
    "rfcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,rf2_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(estimator=rf2, X=X_test, y=y_test, ax=ax, display_labels=[\"No\",\"Yes\"])\n",
    "ax.set_title('Confusion matrix of the classifier', size=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "RocCurveDisplay.from_estimator(estimator=rf2, X=X_test, y=y_test, ax=ax)\n",
    "ax.set_title('ROC Curve of the classifier', size=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a feature importance graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf2.feature_importances_\n",
    "\n",
    "feature_importances = pd.Series(importances, index=X.columns)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "feature_importances.plot.bar(ax=ax, figsize=(10,5))\n",
    "\n",
    "ax.set_title(\"Random Forest Feature Importances\")\n",
    "ax.tick_params('x', rotation=30)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_df = pd.DataFrame(feature_importances, columns=[\"importances\"])\n",
    "feature_importances_df = feature_importances_df.sort_values(by='importances')\n",
    "feature_importances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "sns.barplot(data=feature_importances_df, x=feature_importances_df.importances, y=feature_importances_df.index, orient='h')\n",
    "\n",
    "ax.set_title(\"Decision Tree: Feature Importances for Employee Leaving\", fontsize=15)\n",
    "\n",
    "ax.set_xlabel(\"Importance\")\n",
    "ax.set_ylabel(\"Feature\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting (Future Expansion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the classifier\n",
    "gbc = GradientBoostingClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'loss' : ('log_loss', 'deviance', 'exponential'),\n",
    "              'learning_rate': np.arange(0.0,1.2,0.2),\n",
    "              'n_estimators': np.arange(50,250,50),\n",
    "              'criterion': ('friedman_mse', 'squared_error'),\n",
    "              'max_depth': np.arange(2,11,2),\n",
    "              'min_samples_leaf': np.arange(5,25,5),\n",
    "              'min_samples_split': np.arange(2,25,5)\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a dictionary of scoring metrics to capture\n",
    "scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_randm = RandomizedSearchCV(estimator=gbc, param_distributions = parameters, cv = 5, n_iter = 55, \n",
    "                           n_jobs=-1, scoring=scoring, refit='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gbc_randm.fit(X_random_train, y_random_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_randm.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_randm.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_randm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also find the data for all models evaluated\n",
    "\n",
    "results = pd.DataFrame(gbc_randm.cv_results_)\n",
    "\n",
    "print(results.shape)\n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can order the different models based on their performance\n",
    "results.sort_values(by='mean_test_roc_auc', ascending=False, inplace=True)\n",
    "\n",
    "results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "results[[\"param_n_estimators\", \"param_min_samples_split\",\"param_max_depth\", \"param_min_samples_leaf\", \n",
    "         \"param_criterion\", \"mean_test_accuracy\", \"mean_test_recall\",\n",
    "         \"mean_test_precision\",\"mean_test_f1\",\"mean_test_roc_auc\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_results(model_name, model_object):\n",
    "    '''\n",
    "    Accepts as arguments a model name (your choice - string) and\n",
    "    a fit GridSearchCV model object.\n",
    "  \n",
    "    Returns a pandas df with the F1, recall, precision, and accuracy scores\n",
    "    for the model with the best mean F1 score across all validation folds.  \n",
    "    '''\n",
    "\n",
    "    # Get all the results from the CV and put them in a df\n",
    "    cv_results = pd.DataFrame(model_object.cv_results_)\n",
    "\n",
    "    # Isolate the row of the df with the max(mean f1 score)\n",
    "    best_estimator_results = cv_results.iloc[cv_results['mean_test_roc_auc'].idxmax(), :]\n",
    "\n",
    "    # Extract accuracy, precision, recall, and f1 score from that row\n",
    "    f1 = best_estimator_results.mean_test_f1\n",
    "    recall = best_estimator_results.mean_test_recall\n",
    "    precision = best_estimator_results.mean_test_precision\n",
    "    accuracy = best_estimator_results.mean_test_accuracy\n",
    "    rocauc = best_estimator_results.mean_test_roc_auc\n",
    "  \n",
    "    # Create table of results\n",
    "    table = pd.DataFrame()\n",
    "    table = table.append({'Model': model_name,\n",
    "                        'F1': f1,\n",
    "                        'Recall': recall,\n",
    "                        'Precision': precision,\n",
    "                        'Accuracy': accuracy,\n",
    "                        'ROC-AUC' : rocauc  \n",
    "                        },\n",
    "                        ignore_index=True\n",
    "                       )\n",
    "  \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function on our model\n",
    "gbc_result_table = make_results(\"Gradient Boosting RCV\", gbc_randm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_result_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned Gradient Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc2 = GradientBoostingClassifier(n_estimators=100, criterion='squared_error', max_depth=2, loss='deviance',\n",
    "                                 learning_rate= 0.8, min_samples_leaf=10, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc2_pred = gbc2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc2_pred[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = cross_validate(estimator=gbc2, X=X_train, y=y_train, scoring=\"roc_auc\", cv=kf, n_jobs=2, return_train_score=True)\n",
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv[\"train_roc_auc\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv[\"test_roc_auc\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned Gradient Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbcm = confusion_matrix(y_test,gbc2_pred)\n",
    "gbcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, gbc2_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(estimator=gbc2, X=X_test, y=y_test, ax=ax, display_labels=[\"No\",\"Yes\"])\n",
    "ax.set_title('Confusion matrix of the classifier', size=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "RocCurveDisplay.from_estimator(estimator=gbc2, X=X_test, y=y_test, ax=ax)\n",
    "ax.set_title('ROC Curve of the classifier', size=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbc = HistGradientBoostingClassifier(early_stopping='auto', scoring='roc_auc', random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'loss' : ('auto', 'binary_crossentropy', 'categorical_crossentropy'),\n",
    "              'learning_rate': np.arange(0.0,1.1,0.2),\n",
    "              'max_depth': np.arange(2,10,2),\n",
    "              'min_samples_leaf': np.arange(5,21,5),\n",
    "              'max_depth': np.arange(2,11,1)\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a dictionary of scoring metrics to capture\n",
    "scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbc_randm = RandomizedSearchCV(estimator=hgbc, param_distributions = parameters, cv = 5, n_iter = 55, \n",
    "                           n_jobs=2, scoring=scoring, refit='roc_auc', return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hgbc_randm.fit(X_random_train, y_random_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbc_randm.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbc_randm.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbc_randm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also find the data for all models evaluated\n",
    "\n",
    "results = pd.DataFrame(hgbc_randm.cv_results_)\n",
    "\n",
    "print(results.shape)\n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can order the different models based on their performance\n",
    "results.sort_values(by='mean_test_roc_auc', ascending=False, inplace=True)\n",
    "\n",
    "results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "results[[\"param_loss\", \"param_max_depth\", \"param_min_samples_leaf\", \n",
    "         \"param_learning_rate\", \"mean_test_accuracy\", \"mean_test_recall\",\n",
    "         \"mean_test_precision\",\"mean_test_f1\",\"mean_test_roc_auc\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_results(model_name, model_object):\n",
    "    '''\n",
    "    Accepts as arguments a model name (your choice - string) and\n",
    "    a fit GridSearchCV model object.\n",
    "  \n",
    "    Returns a pandas df with the F1, recall, precision, and accuracy scores\n",
    "    for the model with the best mean F1 score across all validation folds.  \n",
    "    '''\n",
    "\n",
    "    # Get all the results from the CV and put them in a df\n",
    "    cv_results = pd.DataFrame(model_object.cv_results_)\n",
    "\n",
    "    # Isolate the row of the df with the max(mean f1 score)\n",
    "    best_estimator_results = cv_results.iloc[cv_results['mean_test_roc_auc'].idxmax(), :]\n",
    "\n",
    "    # Extract accuracy, precision, recall, and f1 score from that row\n",
    "    f1 = best_estimator_results.mean_test_f1\n",
    "    recall = best_estimator_results.mean_test_recall\n",
    "    precision = best_estimator_results.mean_test_precision\n",
    "    accuracy = best_estimator_results.mean_test_accuracy\n",
    "    rocauc = best_estimator_results.mean_test_roc_auc\n",
    "  \n",
    "    # Create table of results\n",
    "    table = pd.DataFrame()\n",
    "    table = table.append({'Model': model_name,\n",
    "                        'F1': f1,\n",
    "                        'Recall': recall,\n",
    "                        'Precision': precision,\n",
    "                        'Accuracy': accuracy,\n",
    "                        'ROC-AUC' : rocauc  \n",
    "                        },\n",
    "                        ignore_index=True\n",
    "                       )\n",
    "  \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function on our model\n",
    "hist_result_table = make_results(\"HistGradientBoostingClassifier RCV\", hgbc_randm)\n",
    "\n",
    "hist_result_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HistGradientBoosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbc = HistGradientBoostingClassifier(min_samples_leaf=10, max_depth=4, loss='binary_crossentropy',\n",
    "                                     learning_rate=0.4, early_stopping='auto', scoring='roc_auc', random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbc_pred = hgbc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbc_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = cross_validate(estimator=hgbc, X=X_train, y=y_train, scoring=\"roc_auc\", cv=kf, n_jobs=2, return_train_score=True)\n",
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv[\"train_roc_auc\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv[\"test_roc_auc\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned Hist Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbcm = confusion_matrix(y_test, hgbc_pred)\n",
    "hgbcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, hgbc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(estimator=hgbcm, X=X_test, y=y_test, ax=ax, display_labels=[\"No\",\"Yes\"])\n",
    "ax.set_title('Confusion matrix of the classifier', size=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "RocCurveDisplay.from_estimator(estimator=hgbcm, X=X_test, y=y_test, ax=ax)\n",
    "ax.set_title('ROC Curve of the classifier', size=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a feature importance graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permimpt = permutation_importance(estimator=hgbc, X=X_train, y=y_train, scoring=\"roc_auc\", n_repeats=5,\n",
    "                       n_jobs=None, random_state=0)\n",
    "\n",
    "permimpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbc_importances = pd.Series(data=permimpt[\"importances_mean\"], index=X.columns)\n",
    "hgbc_sorted = hgbc_importances.sort_values(ascending=False)\n",
    "\n",
    "hgbc_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "sns.barplot(hgbc_sorted.index, hgbc_sorted.values)\n",
    "\n",
    "ax.set_title(\"Hist Gradient Boosting Features Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catboost (Future Expansion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb1 = CatBoostClassifier(random_seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'learning_rate': np.arange(0.0,1.1,0.2),\n",
    "              'max_depth': np.arange(2,10,2),\n",
    "              'min_samples_leaf': np.arange(5,21,5),\n",
    "              'max_depth': np.arange(2,11,1)\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a dictionary of scoring metrics to capture\n",
    "scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbc_randm = RandomizedSearchCV(estimator=hgbc, param_distributions = parameters, cv = 5, n_iter = 55, \n",
    "                           n_jobs=2, scoring=scoring, refit='roc_auc', return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hgbc_randm.fit(X_random_train, y_random_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbc_randm.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbc_randm.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbc_randm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also find the data for all models evaluated\n",
    "\n",
    "results = pd.DataFrame(hgbc_randm.cv_results_)\n",
    "\n",
    "print(results.shape)\n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can order the different models based on their performance\n",
    "results.sort_values(by='mean_test_roc_auc', ascending=False, inplace=True)\n",
    "\n",
    "results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "results[[\"param_loss\", \"param_max_depth\", \"param_min_samples_leaf\", \n",
    "         \"param_learning_rate\", \"mean_test_accuracy\", \"mean_test_recall\",\n",
    "         \"mean_test_precision\",\"mean_test_f1\",\"mean_test_roc_auc\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_results(model_name, model_object):\n",
    "    '''\n",
    "    Accepts as arguments a model name (your choice - string) and\n",
    "    a fit GridSearchCV model object.\n",
    "  \n",
    "    Returns a pandas df with the F1, recall, precision, and accuracy scores\n",
    "    for the model with the best mean F1 score across all validation folds.  \n",
    "    '''\n",
    "\n",
    "    # Get all the results from the CV and put them in a df\n",
    "    cv_results = pd.DataFrame(model_object.cv_results_)\n",
    "\n",
    "    # Isolate the row of the df with the max(mean f1 score)\n",
    "    best_estimator_results = cv_results.iloc[cv_results['mean_test_roc_auc'].idxmax(), :]\n",
    "\n",
    "    # Extract accuracy, precision, recall, and f1 score from that row\n",
    "    f1 = best_estimator_results.mean_test_f1\n",
    "    recall = best_estimator_results.mean_test_recall\n",
    "    precision = best_estimator_results.mean_test_precision\n",
    "    accuracy = best_estimator_results.mean_test_accuracy\n",
    "    rocauc = best_estimator_results.mean_test_roc_auc\n",
    "  \n",
    "    # Create table of results\n",
    "    table = pd.DataFrame()\n",
    "    table = table.append({'Model': model_name,\n",
    "                        'F1': f1,\n",
    "                        'Recall': recall,\n",
    "                        'Precision': precision,\n",
    "                        'Accuracy': accuracy,\n",
    "                        'ROC-AUC' : rocauc  \n",
    "                        },\n",
    "                        ignore_index=True\n",
    "                       )\n",
    "  \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function on our model\n",
    "hist_result_table = make_results(\"HistGradientBoostingClassifier RCV\", hgbc_randm)\n",
    "\n",
    "hist_result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned Catboost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb2 = CatBoostClassifier(random_seed=0, loss_function=\"Logloss\", early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb2.fit(X_train,y_train, cat_features=[\"professionaldriver\",\"android\"], verbose=False, plot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb2pred = cb2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb2pred[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned Catboost Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catcm = confusion_matrix(y_test, cb2pred)\n",
    "catcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, cb2pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(estimator=cb2, X=X_test, y=y_test, ax=ax, display_labels=[\"No\",\"Yes\"])\n",
    "ax.set_title('Confusion matrix of the classifier', size=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "RocCurveDisplay.from_estimator(estimator=cb2, X=X_test, y=y_test, ax=ax)\n",
    "ax.set_title('ROC Curve of the classifier', size=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb2.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_importances = pd.Series(data=cb2.feature_importances_, index=X.columns)\n",
    "catboost_sorted = catboost_importances.sort_values(ascending=False)\n",
    "catboost_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "sns.barplot(y = catboost_sorted.index, x = catboost_sorted.values)\n",
    "\n",
    "ax.set_title(\"CatBoost Features Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost (Scikit-Learn)\n",
    "\n",
    "XGBoost is a widespread implementation of gradient boosting. Let’s discuss some features of XGBoost that make it so attractive.\n",
    "\n",
    "- XGBoost offers regularization, which allows you to control overfitting by introducing L1/L2 penalties on the weights and biases of each tree. This feature is not available in many other implementations of gradient boosting.\n",
    "- Another feature of XGBoost is its ability to handle sparse data sets using the weighted quantile sketch algorithm. This algorithm allows us to deal with non-zero entries in the feature matrix while retaining the same computational complexity as other algorithms like stochastic gradient descent.\n",
    "- XGBoost also has a block structure for parallel learning. It makes it easy to scale up on multicore machines or clusters. It also uses cache awareness, which helps reduce memory usage when training models with large datasets.\n",
    "- Finally, XGBoost offers out-of-core computing capabilities using disk-based data structures instead of in-memory ones during the computation phase.\n",
    "\n",
    "\n",
    "<img src = \"treesalgo.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validated hyperparameter tuning\n",
    "\n",
    "The cross-validation process is the same as it was for the decision tree and random forest models. The only difference is that we're tuning different hyperparameters now. The steps are included below as a review. \n",
    "\n",
    "For details on cross-validating with `GridSearchCV`, refer back to the [decision tree notebook](https://colab.sandbox.google.com/drive/164Aa1ODOMSIY_5-ZP1PA5afGegTqqjcv?resourcekey=0-hZwiQ1rxwUAol5kaj7-o4w#tuning), or to the [GridSearchCV documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) in scikit-learn.\n",
    "\n",
    "1. Instantiate the classifier (and set the `random_state`). Note here that we've included a parameter called `objective` whose value is `binary:logistic`. This means that the model is performing a binary classification task that outputs a logistic probability. The objective would be different for different kinds of problems—for instance, if you were trying to predict more than two classes or performing a linear regression on continuous data. Refer to the [XGBoost documentation](https://xgboost.readthedocs.io/en/stable/parameter.html) for more information.\n",
    "\n",
    "2. Create a dictionary of hyperparameters to search over.\n",
    "\n",
    "3. Create a dictionary of scoring metrics to capture. \n",
    "\n",
    "4. Instantiate the `GridSearchCV` object. Pass as arguments:\n",
    "  - The classifier (`xgb`)\n",
    "  - The dictionary of hyperparameters to search over (`cv_params`)\n",
    "  - The dictionary of scoring metrics (`scoring`)\n",
    "  - The number of cross-validation folds you want (`cv=5`)\n",
    "  - The scoring metric that you want GridSearch to use when it selects the \"best\" model (i.e., the model that performs best on average over all validation folds) (`refit='f1'`)\n",
    "\n",
    "5. Fit the data (`X_train`, `y_train`) to the `GridSearchCV` object (`xgb_cv`)\n",
    "\n",
    "Note that we use the `%%time` magic at the top of the cell where we fit the model. This outputs the final runtime of the cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc1 = XGBClassifier(random_state=0, n_estimators=100, objective='binary:logistic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth': np.arange(2,10,2),\n",
    "              'learning_rate': np.arange(0.1,0.5,0.1),\n",
    "              'n_estimators':np.arange(50,350,50),\n",
    "              'min_child_weight': [1,2,3,4,5]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbgs = GridSearchCV(estimator=xgbc,param_grid=parameters,scoring=scoring,\n",
    "                     n_jobs=2, cv=5, verbose=1, refit='roc_auc', return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xgbgs.fit(X_random_train,y_random_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbgs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbgs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbgs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_results(model_name, model_object):\n",
    "    '''\n",
    "    Accepts as arguments a model name (your choice - string) and\n",
    "    a fit GridSearchCV model object.\n",
    "  \n",
    "    Returns a pandas df with the F1, recall, precision, and accuracy scores\n",
    "    for the model with the best mean F1 score across all validation folds.  \n",
    "    '''\n",
    "\n",
    "    # Get all the results from the CV and put them in a df\n",
    "    cv_results = pd.DataFrame(model_object.cv_results_)\n",
    "\n",
    "    # Isolate the row of the df with the max(mean f1 score)\n",
    "    best_estimator_results = cv_results.iloc[cv_results['mean_test_roc_auc'].idxmax(), :]\n",
    "\n",
    "    # Extract accuracy, precision, recall, and f1 score from that row\n",
    "    f1 = best_estimator_results.mean_test_f1\n",
    "    recall = best_estimator_results.mean_test_recall\n",
    "    precision = best_estimator_results.mean_test_precision\n",
    "    accuracy = best_estimator_results.mean_test_accuracy\n",
    "    rocauc = best_estimator_results.mean_test_roc_auc\n",
    "  \n",
    "    # Create table of results\n",
    "    table = pd.DataFrame()\n",
    "    table = table.append({'Model': model_name,\n",
    "                        'F1': f1,\n",
    "                        'Recall': recall,\n",
    "                        'Precision': precision,\n",
    "                        'Accuracy': accuracy,\n",
    "                        'ROC-AUC' : rocauc  \n",
    "                        },\n",
    "                        ignore_index=True\n",
    "                       )\n",
    "  \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create xgb model results table\n",
    "xgb_cv_results = make_results('XGBoost GSCV', xgbgs)\n",
    "xgb_cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc = XGBClassifier(random_state=0, n_estimators=100, objective='binary:logistic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgbc = XGBClassifier(random_state=0, n_estimators=100, objective='softmax:multi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth': np.arange(2,11,1),\n",
    "              'eta': [0.01, 0.1, 0.5, 1.0],\n",
    "              'n_estimators': np.arange(50,300,50),\n",
    "              'min_child_weight': np.arange(1,4,1),\n",
    "              'gamma': np.arange(0,11,2),\n",
    "              'subsample': np.arange(0.1, 0.9, 0.1),\n",
    "              'colsample_bytree': np.arange(0.5,0.9,0.1),\n",
    "              'reg_alpha': np.arange(0.0 , 1.0, 0.2),\n",
    "              'reg_lambda': np.arange(0.0, 1.0, 0.2)\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbrandm = RandomizedSearchCV(estimator=xgbc, param_distributions = parameters, cv = 5, n_iter = 55, \n",
    "                           n_jobs=2, scoring=scoring, refit='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbrandm.fit(X_random_train, y_random_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbrandm.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbrandm.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbrandm.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc2 = XGBClassifier(n_estimators=100, objective='binary:logistic', random_state=0, \n",
    "                     colsample_bytree=0.5, eta=0.01, gamma=6, min_child_weight=2, max_depth=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgbmodel = XGBClassifier(random_state=0, n_estimators=100, objective='softmax:multi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgbmodel.fit(X_train,y_train,eval_set=[(X_test,y_test)],eval_metric='rmse',early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc2.fit(X_train, y_train, eval_set=[(X_test,y_test)], eval_metric='error', early_stopping_rounds=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgbmodel.fit(X_train_scaled,y_train,eval_set=[(X_test_scaled,y_test)],eval_metric='error',early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgbmodel.fit(X_train_scaled,y_train,eval_set=[(X_test_scaled,y_test)],eval_metric='mlogloss',early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc2_pred = xgbc2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc2_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbcm = confusion_matrix( y_test, xgbc2_pred)\n",
    "xgbcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,xgbc2_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(estimator=xgbc2, X=X_test, y=y_test, ax=ax, display_labels=[\"No\",\"Yes\"])\n",
    "ax.set_title('Confusion matrix of the classifier', size=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "RocCurveDisplay.from_estimator(estimator=xgbc2, X=X_test, y=y_test, ax=ax)\n",
    "ax.set_title('ROC Curve of the classifier', size=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Feature importance\n",
    "\n",
    "The XGBoost library has a function called `plot_importance`, which we imported at the beginning of this notebook. This let's us check the features selected by the model as the most predictive. We can create a plot by calling this function and passing to it the best estimator from our grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "plot_importance(xgbrandm.best_estimator_, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = pd.Series(xgbmodel.feature_importances_, index=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances.nlargest(10).plot(kind='barh', figsize=(10,8))\n",
    "plt.title('Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The permutation based importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_importance = permutation_importance(rf,X_test,y_test, random_state=0, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idx = perm_importance.importances_mean.argsort()\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title(\"Permutation-based Importance\")\n",
    "plt.barh(X.columns[sorted_idx], perm_importance.importances_mean[sorted_idx])\n",
    "plt.xlabel(\"Permutation Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Importance from SHAP Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost (Native Booster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(data=X_train,label=y_train)\n",
    "dtest = xgb.DMatrix(data=X_test,label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_estimators':,\n",
    "          'learning_rate':,\n",
    "          'max_depth':,\n",
    "          'objective': '',\n",
    "          'num_class': ,\n",
    "          'seed': 0,\n",
    "          'eval_metric':''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbmodel = xgb.train(params=params,dtrain=dtrain,num_boost_round=100,evals=[(dtest,\"Test\")],\n",
    "                     early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgbmodel.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation (API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = xgb.cv(params=params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=100,\n",
    "            nfold=5,\n",
    "            stratified=False,\n",
    "            folds=None,\n",
    "            metrics=('merror'),\n",
    "            obj=None,\n",
    "            feval=None,\n",
    "            maximize=False,\n",
    "            early_stopping_rounds=10,\n",
    "            fpreproc=None,\n",
    "            as_pandas=True,\n",
    "            verbose_eval=None,\n",
    "            show_stdv=True,\n",
    "            seed=0,\n",
    "            callbacks=None,\n",
    "            shuffle=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv['test-merror-mean'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test,y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , ax = plt.subplots(figsize=(10,5))\n",
    "sns.heatmap(cm, annot=True,fmt='.4g',linewidths=2, cmap='viridis')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , ax = plt.subplots(figsize=(10,5))\n",
    "plot_confusion_matrix(xgbmodel,X_test_scaled,y_test,values_format='.4g',ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(xgbmodel,X_test,y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Formatted View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table[\"True Value\"] = y_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table[\"Predicted\"] = np.round(lr_pred,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = pd.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = xgbmodel.predict(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare models\n",
    "\n",
    "Create a table of results to compare model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table of results to compare model performance.\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "table = pd.DataFrame()\n",
    "table = table.append({'Model': \"Tuned Decision Tree\",\n",
    "                        'F1':  0.945422,\n",
    "                        'Recall': 0.935863,\n",
    "                        'Precision': 0.955197,\n",
    "                        'Accuracy': 0.940864\n",
    "                      },\n",
    "                        ignore_index=True\n",
    "                    )\n",
    "\n",
    "table = table.append({'Model': \"Tuned Random Forest\",\n",
    "                        'F1':  0.947306,\n",
    "                        'Recall': 0.944501,\n",
    "                        'Precision': 0.950128,\n",
    "                        'Accuracy': 0.942450\n",
    "                      },\n",
    "                        ignore_index=True\n",
    "                    )\n",
    "\n",
    "table = table.append({'Model': \"Tuned XGBoost\",\n",
    "                        'F1':  f1_score,\n",
    "                        'Recall': rc_score,\n",
    "                        'Precision': pc_score,\n",
    "                        'Accuracy': ac_score\n",
    "                      },\n",
    "                        ignore_index=True\n",
    "                    )\n",
    "\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "\n",
    "Feature selection is the process of choosing features to be used for modeling. In practice, feature selection takes place at multiple points in the PACE process. Although sometimes you will be given a dataset and a defined target variable, most often in practice you will begin with only a question or a problem that you are tasked with solving. In these cases, if you decide that the problem requires a model, you'll then have to:\n",
    "\n",
    "* Consider what data is available to you\n",
    "* Decide on what kind of model you need\n",
    "* Decide on a target variable\n",
    "* Assemble a collection of features that you think might help predict on your chosen target\n",
    "\n",
    "This would all take place during the **Plan** phase. \n",
    "\n",
    "Then, during the **Analyze** phase, you would perform EDA on the data and reevaluate your variables for appropriateness. For example, can your model handle null values? If not, what do you do with features with a lot of nulls? Perhaps you drop them. This too is feature selection.\n",
    "\n",
    "But it doesn't end there. Feature selection also occurs during the **Construct** phase. This usually involves building a model, examining which features are most predictive, and then removing the unpredictive features.\n",
    "\n",
    "There's a lot of work involved in feature selection. In our case, we already have a dataset, and we're not performing thorough EDA on it. But we can still examine the data to ensure that all the features can reasonably be expected to have predictive potential. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Methods (Basics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Threshold (Numeric Only)\n",
    "\n",
    "Remember we should apply the variance filter only on numerical variables.\n",
    "\n",
    "Default Value of Threshold is 0\n",
    "\n",
    "    If Variance Threshold = 0 (Remove Constant Features )\n",
    "    If Variance Threshold > 0 (Remove Quasi-Constant Features )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold_n=0.95\n",
    "\n",
    "# vt = VarianceThreshold(threshold=(threshold_n* (1 - threshold_n) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vtfit = vt.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vtfit.variances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vt.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vt.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant and Quasi-constant features with Feature-engine\n",
    "\n",
    "In this notebook, we will remove constant and quasi-constant features utilizing the new functionality from Feature-engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,0:17]\n",
    "y = df.iloc[:,17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove constant features\n",
    "\n",
    "The DropConstantFeatures class from Feature-engine finds and removes constant and quasi-constant features from a dataset. We can remove constant features by setting the parameter tol to 1, or quasi-constant with smaller values for tol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = DropConstantFeatures(tol=1, variables=None, missing_values='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of constant features\n",
    "\n",
    "sel.features_to_drop_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove constant features from the data\n",
    "\n",
    "X_train = sel.transform(X_train)\n",
    "X_test = sel.transform(X_test)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove quasi-constant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = DropConstantFeatures(tol=0.90, variables=None, missing_values='raise') #90% majority observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of quasi-constant features\n",
    "\n",
    "sel.features_to_drop_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of observations showing each of the different values\n",
    "# of the variable\n",
    "\n",
    "var = sel.features_to_drop_[0]\n",
    "\n",
    "X_train[var].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the quasi-constant features\n",
    "\n",
    "X_train = sel.transform(X_train)\n",
    "X_test = sel.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicated features with Feature-engine\n",
    "\n",
    "In this notebook, we will identify and remove duplicated features with Feature-engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the selector\n",
    "sel = DropDuplicateFeatures(variables=None, missing_values='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the duplicate features, this might take a while\n",
    "sel.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the pairs of duplicated features\n",
    "# each set are duplicates\n",
    "\n",
    "sel.duplicated_feature_sets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the features that will be dropped\n",
    "# 1 from each of the pairs above\n",
    "\n",
    "sel.features_to_drop_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the duplicated features\n",
    "\n",
    "X_train = sel.transform(X_train)\n",
    "X_test = sel.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Methods (Correlation)\n",
    "\n",
    "Correlation Feature Selection evaluates subsets of features on the basis of the following hypothesis: \"Good feature subsets contain features highly correlated with the target, yet uncorrelated to each other\".\n",
    "\n",
    "### Correlation with Feature-engine\n",
    "\n",
    "- The DropCorrelatedFeatures class from Feature-engine does a similar job to the brute force approach that we described earlier.\n",
    "\n",
    "- The SmartCorrelationSelection allows us to select a feature from each correlated group based on model performance, number of missing values, cardinality or variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the selector\n",
    "\n",
    "sel = DropCorrelatedFeatures(\n",
    "    threshold=0.8,\n",
    "    method='pearson',\n",
    "    missing_values='ignore'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find correlated features\n",
    "\n",
    "sel.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each set contains a group of correlated features\n",
    "\n",
    "sel.correlated_feature_sets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop correlated features\n",
    "\n",
    "X_train = sel.transform(X_train)\n",
    "X_test = sel.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SmartCorrelationSelection\n",
    "\n",
    "### Model Performance\n",
    "\n",
    "We will keep a feature from each correlation group based on the performance of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation selector\n",
    "sel = SmartCorrelatedSelection(\n",
    "    variables=None, # if none, selector examines all numerical variables\n",
    "    method=\"pearson\",\n",
    "    threshold=0.8,\n",
    "    missing_values=\"raise\",\n",
    "    selection_method=\"model_performance\",\n",
    "    estimator=lr,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this may take a while, because we are training\n",
    "# a random forest per correlation group\n",
    "\n",
    "sel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups of correlated features\n",
    "\n",
    "sel.correlated_feature_sets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this group, several features are highly correlated. Which one should we keep and which ones should we remove?**\n",
    "\n",
    "One criteria to select which features to use from this group, would be to use those with **less missing data**. \n",
    "\n",
    "Our dataset contains no missing values, so this is not an option. But keep this in mind when you work with your own datasets.\n",
    "\n",
    "**Note**\n",
    "\n",
    "None of the 2 procedures for removing correlated features are perfect, and some correlated features may escape the loops of code. So it might be worthwhile checking that after removing the correlated features, there are no correlated features left in the dataset. If there are, repeat the procedure to remove the remaining ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Methods (Statistical Tests)\n",
    "\n",
    "## Mutual information\n",
    "\n",
    "The mutual information measures the reduction in uncertainty in variable A when variable B is known. \n",
    "\n",
    "To select variables, we are interested in the mutual information between the predictor variables and the target. Higher mutual information values, indicate little uncertainty about the target Y given the predictor X.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the mutual information\n",
    "mi = mutual_info_classif(X_train, y_train)\n",
    "\n",
    "# and make a bar  plot\n",
    "mi = pd.Series(mi)\n",
    "mi.index = X_train.columns\n",
    "mi.sort_values(ascending=False).plot.bar(figsize=(20,6))\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Mutual Information')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we will select the top 10 features\n",
    "# based on their mutual information value\n",
    "\n",
    "# select features\n",
    "selkbest = SelectKBest(mutual_info_classif, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selkbest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display features\n",
    "X_train.columns[selkbest.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove the rest of the features:\n",
    "\n",
    "X_train = selkbest.transform(X_train)\n",
    "X_test = selkbest.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape    # Can start training ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features in the top percentile\n",
    "selpercent = SelectPercentile(mutual_info_classif, percentile=30) # Based on no of features to decide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selpercent.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the features\n",
    "X_train.columns[selpercent.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove the rest of the features:\n",
    "\n",
    "X_train = selpercent.transform(X_train)\n",
    "X_test = selpercent.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate feature selection\n",
    "\n",
    "Univariate feature selection works by selecting the best features based on univariate statistical tests (ANOVA). The methods estimate the degree of linear dependency between two random variables. In this case, any of the predictor variables and the target. \n",
    "\n",
    "ANOVA assumes a linear relationship between the feature and the target and that the variables follow a Gaussian distribution. If this is not true, the result of this test may not be useful.\n",
    "\n",
    "These may not always be the case for the variables in your dataset, so if looking to implement these procedure, you will need to corroborate these assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate anova\n",
    "univariate = f_classif(X_train, y_train)\n",
    "\n",
    "# plot values\n",
    "univariate = pd.Series(univariate[1])\n",
    "univariate.index = X_train.columns\n",
    "univariate.sort_values(ascending=False).plot.bar(figsize=(20,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the top 10 features\n",
    "selkbest = SelectKBest(f_classif, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selkbest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display selected feature names\n",
    "X_train.columns[selkbest.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove the rest of the features:\n",
    "\n",
    "X_train = selpercent.transform(X_train)\n",
    "X_test = selpercent.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features in top 10th percentile\n",
    "selpercent= SelectPercentile(f_classif, percentile=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selpercent.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display selected feature names\n",
    "X_train.columns[selpercent.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove the rest of the features:\n",
    "\n",
    "X_train = selpercent.transform(X_train)\n",
    "X_test = selpercent.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Methods (Other Methods)\n",
    "\n",
    "## Univariate Performance with Feature-engine\n",
    "\n",
    "This procedure works as follows:\n",
    "\n",
    "- Train a ML model per every single feature\n",
    "- Determine the performance of the models\n",
    "- Select features if model performance is above a certain threshold\n",
    "\n",
    "The C value in Logistic Regression is an user adjustable parameter that controls regularisation. In simple terms, higher values of C will instruct our model to fit the training set as best as possible, while lower C values will favour a simple models with coefficients closer to zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the machine learning model\n",
    "lr = LogisticRegression(penalty='l2', C=1000, random_state=0, solver='lbfgs', max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the selector\n",
    "sel = SelectBySingleFeaturePerformance(\n",
    "    variables=None,\n",
    "    estimator=lr,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=5,\n",
    "    threshold=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find predictive features\n",
    "sel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the transformer stores a dictionary of feature:metric pairs\n",
    "# notice that the roc can be positive or negative.\n",
    "# the selector selects based on the absolute value\n",
    "\n",
    "#In general, an AUC of 0.5 suggests no discrimination \n",
    "#(i.e., ability to diagnose patients with and without the disease or condition based on the test), \n",
    "#0.7 to 0.8 is considered acceptable, 0.8 to 0.9 is considered excellent, and more than 0.9 is considered outstanding\n",
    "\n",
    "sel.feature_performance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(sel.feature_performance_).sort_values(ascending=False).plot.bar(figsize=(20, 5))\n",
    "plt.title('Performance of ML models trained with individual features', size=15)\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('ROC Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same plot but taking the absolute value of the r2\n",
    "\n",
    "# np.abs(pd.Series(sel.feature_performance_)).sort_values(ascending=False).plot.bar(figsize=(20, 5))\n",
    "# plt.title('Performance of ML models trained with individual features', size=15)\n",
    "# plt.ylabel('ROC Score')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the features that will be removed\n",
    "\n",
    "sel.features_to_drop_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features in the dataframes\n",
    "\n",
    "X_train = sel.transform(X_train)\n",
    "X_test = sel.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step forward feature selection\n",
    "\n",
    "Step forward feature selection starts by training a machine learning model for each feature in the dataset and selecting, as the starting feature, the one that returns the best performing model according to the evaluation criteria we choose.\n",
    "\n",
    "In the second step, it creates machine learning models for all combinations of the feature selected in the previous step and a second feature. It selects the pair that produces the best performing algorithm.\n",
    "\n",
    "It continues by adding 1 feature at a time to the features that were selected in previous steps, until a stopping criteria is reached.\n",
    "\n",
    "In theory, models with more features perform better. The algorithm will continue adding new features until a certain criteria is met. For example, until the model performance does not increase beyond a certain threshold. Or, as we show in this notebook, until a certain number of features are selected.\n",
    "\n",
    "The model performance metric can be the roc_auc for classification and the r squared for regression, for example, and it is determined by the user. \n",
    "\n",
    "Step forward feature selection is called a greedy procedure because it evaluates many possible single, double, triple, and so on feature combinations. Therefore, it is very computationally expensive and, sometimes, if the feature space is big enough, even unfeasible.\n",
    "\n",
    "Scikit-learn provides various stopping criteria to stop the search:\n",
    "\n",
    "* when a certain number of features is reached (like MLXtend) (arbitrary)\n",
    "* if the performance does not increase beyond a certain threshold (ideal but expensive)\n",
    "* selects half of the features (arbitrary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Forward Feature Selection Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"carpricemod.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,0:14]\n",
    "y = df.iloc[:,14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.values, y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# within the SFS we indicate:\n",
    "\n",
    "# 1) the algorithm we want to create, in this case RandomForests\n",
    "# (note that I use few trees to speed things up)\n",
    "\n",
    "# 2) the stopping criteria: see sklearn documentation for more details\n",
    "\n",
    "# 3) whether to perform step forward or step backward\n",
    "\n",
    "# 4) the evaluation metric: in this case the roc_auc\n",
    "# 5) the cross-validation\n",
    "\n",
    "# this is going to take a while, do not despair\n",
    "\n",
    "sfs = SFS(estimator=LinearRegression(), \n",
    "          n_features_to_select=6,\n",
    "          direction='forward',\n",
    "          scoring='r2',\n",
    "          cv=5,\n",
    "          n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs = sfs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs.n_features_to_select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Forward Feature Selection Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,0:8]\n",
    "y = df.iloc[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.values, y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# within the SFS we indicate:\n",
    "\n",
    "# 1) the algorithm we want to create, in this case RandomForests\n",
    "# (note that I use few trees to speed things up)\n",
    "\n",
    "# 2) the stopping criteria: see sklearn documentation for more details\n",
    "\n",
    "# 3) whether to perform step forward or step backward\n",
    "\n",
    "# 4) the evaluation metric: in this case the roc_auc\n",
    "# 5) the cross-validation\n",
    "\n",
    "# this is going to take a while, do not despair\n",
    "\n",
    "sfs = SFS(estimator=LogisticRegression(random_state=0), \n",
    "          n_features_to_select=4,\n",
    "          direction='forward',\n",
    "          scoring='f1',\n",
    "          cv=5,\n",
    "          n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs = sfs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step backward feature selection\n",
    "\n",
    "Step Backward Feature Selection starts by fitting a model using all the features in the data set and determining its performance. \n",
    "\n",
    "Then, it trains models on all possible combinations of all features, minus one, and removes the feature that returns the model with the lowest performance.\n",
    "\n",
    "In the third step, it trains models in all possible combinations of the features remaining from step 2, minus 1 feature, and removes the feature that produced the lowest performing model.\n",
    "\n",
    "The algorithm stops when a certain criteria determined by the user is met. This criteria could be that the model performance does not decrease beyond a certain threshold, or alternatively, as we show in this notebook, when we reach a certain number of selected features.\n",
    "\n",
    "The evaluation metric can be the roc_auc for classification or the r squared for regression, for example, and is determined by the user.\n",
    "\n",
    "Step Backward Feature Selection is called greedy because it evaluates all possible n, and then n-1 and n-2 and so on feature combinations. Therefore, it is very computationally expensive and sometimes, if the feature space is big enough, even unfeasible.\n",
    "\n",
    "Scikit-learn provides various stopping criteria to stop the search:\n",
    "\n",
    "* when a certain number of features is reached (like MLXtend) (arbitrary)\n",
    "* if the performance does not increase beyond a certain threshold (ideal but expensive)\n",
    "* selects half of the features (arbitrary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Forward Feature Selection Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,0:14]\n",
    "y = df.iloc[:,14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.values, y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# within the SFS we indicate:\n",
    "\n",
    "# 1) the algorithm we want to create, in this case RandomForests\n",
    "# (note that I use few trees to speed things up)\n",
    "\n",
    "# 2) the stopping criteria: see sklearn documentation for more details\n",
    "\n",
    "# 3) whether to perform step forward or step backward\n",
    "\n",
    "# 4) the evaluation metric: in this case the roc_auc\n",
    "# 5) the cross-validation\n",
    "\n",
    "# this is going to take a while, do not despair\n",
    "\n",
    "sfs = SFS(estimator=LinearRegression(), \n",
    "          n_features_to_select=6,\n",
    "          direction='backward',\n",
    "          scoring='r2',\n",
    "          cv=5,\n",
    "          n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs = sfs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs.n_features_to_select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Backward Feature Selection Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,0:8]\n",
    "y = df.iloc[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.values, y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# within the SFS we indicate:\n",
    "\n",
    "# 1) the algorithm we want to create, in this case RandomForests\n",
    "# (note that I use few trees to speed things up)\n",
    "\n",
    "# 2) the stopping criteria: see sklearn documentation for more details\n",
    "\n",
    "# 3) whether to perform step forward or step backward\n",
    "\n",
    "# 4) the evaluation metric: in this case the roc_auc\n",
    "# 5) the cross-validation\n",
    "\n",
    "# this is going to take a while, do not despair\n",
    "\n",
    "sfs = SFS(estimator=LogisticRegression(random_state=0), \n",
    "          n_features_to_select=4,\n",
    "          direction='backward',\n",
    "          scoring='f1',\n",
    "          cv=5,\n",
    "          n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs = sfs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Pipelines simplify the steps of processing the data. We use the module <code>Pipeline</code> to create a pipeline. \n",
    "`Pipeline` lets you chain together multiple operators on your data that both have a `fit` method.\n",
    "\n",
    "### Combine multiple processing steps into a `Pipeline`\n",
    "\n",
    "A pipeline contains a series of steps, where a step is (\"name of step\", actual_model). The \"name of step\" string is only used to help you identify which step you are on, and to allow you to specify parameters at that step.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input = [('scaler', StandardScaler()), ('lr', LinearRegression())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input = [ ('polynomial', PolynomialFeatures(include_bias=False,degree=2)),('ss',StandardScaler() ), ('model',Ridge(alpha=1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Pipeline(Input, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pred = estimator.predict(X_test)\n",
    "lr_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=================================================================================================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle  \n",
    "\n",
    "When models take a long time to fit, you don’t want to have to fit them more than once. If your kernel disconnects or you shut down the notebook and lose the cell’s output, you’ll have to refit the model, which can be frustrating and time-consuming. \n",
    "\n",
    "`pickle` is a tool that saves the fit model object to a specified location, then quickly reads it back in. It also allows you to use models that were fit somewhere else, without having to train them yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Model\n",
    "This step will ***W***rite (i.e., save) the model, in ***B***inary (hence, `wb`), to the folder designated by the above path. In this case, the name of the file we're writing is `rf_cv_model.pickle`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model.sav'\n",
    "dump(xgbnew,open(filename,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we save the model, we'll never have to re-fit it when we run this notebook. Ideally, we could open the notebook, select \"Run all,\" and the cells would run successfully all the way to the end without any model retraining. \n",
    "\n",
    "For this to happen, we'll need to return to the cell where we defined our grid search and comment out the line where we fit the model. Otherwise, when we re-run the notebook, it would refit the model. \n",
    "\n",
    "Similarly, we'll also need to go back to where we saved the model as a pickle and comment out those lines.  \n",
    "\n",
    "Next, we'll add a new cell that reads in the saved model from the folder we already specified. For this, we'll use `rb` (read binary) and be sure to assign the model to the same variable name as we used above, `rf_cv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load(open(filename,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python code done by Dennis Lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
